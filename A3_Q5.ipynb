{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDlsd64qr_eB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "066b5351-566b-48fd-cdd3-a84fe47a392a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.23.1-py2.py3-none-any.whl (205 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=1a8b5cfd03608310c51cd7204255564c3647b104a0dbb2e3d33ecd25f57f8522\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.23.1 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.3\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "!pip install wandb\n",
        "import torch\n",
        "import heapq\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt55\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.ticker as ticker\n",
        "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login(key='b032cc059132c9aac4f1b317f6f9ad007ef9e4d4')\n",
        "wandb.init(project=\"Assignment3_final\",name=\"Question_5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "kaOvyiUvfALC",
        "outputId": "502f1160-914a-4dad-934f-d64b993f1e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs22m083\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230520_082251-pjrnis19</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22m083/Assignment3_Q5/runs/pjrnis19' target=\"_blank\">Question_5</a></strong> to <a href='https://wandb.ai/cs22m083/Assignment3_Q5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22m083/Assignment3_Q5' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_Q5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22m083/Assignment3_Q5/runs/pjrnis19' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_Q5/runs/pjrnis19</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cs22m083/Assignment3_Q5/runs/pjrnis19?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fc7c47b0e80>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9mPrS3WsG6d",
        "outputId": "883d8b30-83d9-492b-d6ae-bb2345127c8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# to mount to the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxnBHzhGsOgF",
        "outputId": "46a2d231-25f9-4811-ddaf-075ac14508c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/aksharantar_dataset/tel\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/MyDrive/aksharantar_dataset/tel/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3s_FFXIBsWKj"
      },
      "outputs": [],
      "source": [
        "val_tsv_file = open(\"/content/drive/MyDrive/aksharantar_dataset/tel/tel_valid.csv\")\n",
        "tsv_file = open(\"/content/drive/MyDrive/aksharantar_dataset/tel/tel_train.csv\")\n",
        "test_tsv_file = open(\"/content/drive/MyDrive/aksharantar_dataset/tel/tel_test.csv\")\n",
        "val_read_tsv = csv.reader(val_tsv_file)\n",
        "read_tsv = csv.reader(tsv_file)\n",
        "test_read_tsv = csv.reader(test_tsv_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MrJuAAbAfqW"
      },
      "outputs": [],
      "source": [
        "train_Y, train_X = [], []\n",
        "\n",
        "for i in read_tsv:   \n",
        "    train_Y.append(i[1])\n",
        "    train_X.append(i[0])\n",
        "\n",
        "test_Y, test_X = [], []\n",
        "for i in test_read_tsv:\n",
        "    test_Y.append(i[1])\n",
        "    test_X.append(i[0])\n",
        "\n",
        "val_Y, val_X = [], []\n",
        "for i in val_read_tsv:\n",
        "    val_Y.append(i[1])\n",
        "    val_X.append(i[0])\n",
        "\n",
        "train_Y, train_X = np.array(train_Y), np.array(train_X)\n",
        "for i in range(train_Y.shape[0]):\n",
        "    train_Y[i] = \"\\t\" + train_Y[i] + \"\\n\"\n",
        "\n",
        "test_Y, test_X = np.array(test_Y), np.array(test_X)\n",
        "for i in range(test_Y.shape[0]):\n",
        "  test_Y[i] = \"\\t\" + test_Y[i] + \"\\n\"\n",
        "\n",
        "val_Y, val_X = np.array(val_Y), np.array(val_X)\n",
        "for i in range(val_Y.shape[0]):\n",
        "   val_Y[i] = \"\\t\" + val_Y[i] + \"\\n\"\n",
        "ch_end, c_end =  \" \", \" \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ROx9eOfLFXG"
      },
      "outputs": [],
      "source": [
        "val_output_corpus, output_corpus, input_corpus, val_input_corpus = set(), set(), set(), set()\n",
        "d_type, td_type = \"int64\", torch.int64\n",
        "\n",
        "for w in val_Y:\n",
        "    for char in w:\n",
        "        if char not in val_output_corpus:\n",
        "            val_output_corpus.add(char)\n",
        "\n",
        "for w in train_Y:\n",
        "    for char in w:\n",
        "        if char not in output_corpus:\n",
        "            output_corpus.add(char)\n",
        "\n",
        "for w in val_X:\n",
        "    for char in w:\n",
        "        if char not in val_input_corpus:\n",
        "            val_input_corpus.add(char)\n",
        "\n",
        "output_corpus.add(ch_end)\n",
        "\n",
        "for w in train_X:\n",
        "    for char in w:\n",
        "        if char not in input_corpus:\n",
        "            input_corpus.add(char)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTqWfqHBLIuq"
      },
      "outputs": [],
      "source": [
        "input_corpus.add(ch_end)\n",
        "t_len = len(train_X)\n",
        "output_corpus, input_corpus = sorted(list(output_corpus)), sorted(list(input_corpus))\n",
        "num_decoder_tokens, num_encoder_tokens = len(output_corpus), len(input_corpus)\n",
        "max_decoder_seq_length, max_encoder_seq_length = max([len(txt) for txt in train_Y]),  max([len(txt) for txt in train_X]) + 2\n",
        "v_len = len(val_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj9IW0A6s4iq",
        "outputId": "39858cd0-85b4-4ff3-82ed-dadb95397d56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sequence length for outputs: 21\n",
            "Number of unique input tokens: 27\n",
            "Max sequence length for inputs: 30\n",
            "Number of unique output tokens: 65\n",
            "Number of samples: 51200\n"
          ]
        }
      ],
      "source": [
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Number of samples:\", len(train_X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhFZBwPbs7RF"
      },
      "outputs": [],
      "source": [
        "output_char_index, input_char_index = dict([(char, i) for i, char in enumerate(output_corpus)]), dict([(char, i) for i, char in enumerate(input_corpus)])\n",
        "\n",
        "\n",
        "target_data, input_data = np.zeros((max_decoder_seq_length,t_len), dtype = d_type), np.zeros((max_encoder_seq_length,t_len), dtype = d_type)\n",
        "t_z = 0 \n",
        "for i, (x, y) in enumerate(zip(train_X, train_Y)):\n",
        "    for t, char in enumerate(x):\n",
        "        input_data[t, i] = input_char_index[char]\n",
        "    t_z = t+1    \n",
        "    input_data[t_z :,i] = input_char_index[ch_end]\n",
        "    t =  0\n",
        "    for t, char in enumerate(y):\n",
        "        target_data[t, i] = output_char_index[char]\n",
        "    t_z = t+1       \n",
        "    target_data[t_z :,i] = output_char_index[ch_end]\n",
        "    \n",
        "target_data_val, input_data_val = np.zeros((max_decoder_seq_length,v_len), dtype= d_type), np.zeros((max_encoder_seq_length,v_len), dtype= d_type)\n",
        "t_z = 0\n",
        "for i, (x, y) in enumerate(zip(val_X, val_Y)):\n",
        "    for t, char in enumerate(x):\n",
        "        t_z = 0\n",
        "        input_data_val[t, i] = input_char_index[char]\n",
        "    t_z = t+1   \n",
        "    input_data_val[t_z :,i] = input_char_index[ch_end]\n",
        "    t_z = 0\n",
        "    for t, char in enumerate(y):\n",
        "        target_data_val[t, i] = output_char_index[char]\n",
        "    t_z = t+1       \n",
        "    target_data_val[t_z :,i] = output_char_index[ch_end]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPcb6IPKs9SY"
      },
      "outputs": [],
      "source": [
        "# convertin numpy arrays to tensors\n",
        "target_data, input_data =  torch.tensor(target_data,dtype = td_type),  torch.tensor(input_data,dtype = td_type)\n",
        "target_data_val, input_data_val = torch.tensor(target_data_val,dtype = td_type), torch.tensor(input_data_val,dtype = td_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding an attention network to my base sequence-to-sequence model**"
      ],
      "metadata": {
        "id": "zmhemSyqOhv4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVRPujkOtC36"
      },
      "outputs": [],
      "source": [
        "output_size, input_size_decoder, input_size_encoder = num_decoder_tokens, num_decoder_tokens, num_encoder_tokens\n",
        "dec_dropout, enc_dropout = 0.1, 0.1\n",
        "num_epochs, batch_size, learning_rate= 10,32,0.001\n",
        "load_model, training = False, False\n",
        "num_dec_layers, num_enc_layers = 1, 1\n",
        "decoder_embedding_size, encoder_embedding_size, hidden_size = 256, 256, 256 # Needs to be the same for both RNN's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7D9ePImtEY6"
      },
      "outputs": [],
      "source": [
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_target_char_index, reverse_input_char_index = dict((i, char) for char, i in output_char_index.items()), dict((i, char) for char, i in input_char_index.items())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM**"
      ],
      "metadata": {
        "id": "ho3vFB6wM1Jc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2xJHFcstMEm"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        drop_par, emb_size = dropout, embedding_size\n",
        "        self.dropout = nn.Dropout(drop_par)\n",
        "        self.num_layers, inp, self.hidden_size = num_layers, input_size, hidden_size\n",
        "        lstm_par = hidden_size * 2 + emb_size\n",
        "        self.embedding = nn.Embedding(inp, emb_size)\n",
        "        lin_par = hidden_size * 3\n",
        "        self.rnn = nn.LSTM(lstm_par, hidden_size, num_layers)\n",
        "        \n",
        "        self.energy = nn.Linear(lin_par, 1)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.Softmax( dim = 0)\n",
        "        self.relu = nn.Hardshrink()\n",
        "\n",
        "    def forward(self, x, encoder_states, hidden, cell):\n",
        "        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
        "        # is 1 here because we are sending in a single word and not a sentence\n",
        "        x, _dim  = x.unsqueeze(0), 2\n",
        "        drop_par, sequence_length = self.embedding(x), encoder_states.shape[0]\n",
        "        embedding = self.dropout(drop_par)\n",
        "        # embedding shape: (1, N, embedding_size)\n",
        "        \n",
        "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
        "        # h_reshaped: (seq_length, N, hidden_size*2)\n",
        "        relu_par = torch.cat((h_reshaped, encoder_states), dim = _dim)\n",
        "        energy = self.relu(self.energy(relu_par))\n",
        "        # energy: (seq_length, N, 1)\n",
        "        \n",
        "        attention = self.softmax(energy)\n",
        "        # attention: (seq_length, N, 1)\n",
        "        \n",
        "        encoder_states = encoder_states.permute(1,0,2)\n",
        "\n",
        "        attention = attention.permute(1,2,0)\n",
        "        # attention: (N, seq_length, 1)\n",
        "\n",
        "        encoder_states = self.relu(encoder_states)\n",
        "        # encoder_states: (N, seq_length, hidden_size*2)\n",
        "        context_init = torch.bmm(attention, encoder_states)\n",
        "        context_vector = context_init.permute(1,0,2)\n",
        "        \n",
        "        rnn_input = torch.cat((context_vector,embedding),dim = _dim)\n",
        "       # rnn_input = self.relu(rnn_input)\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "        # outputs shape: (1, N, hidden_size)\n",
        "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
        "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
        "        # just gonna remove the first dim\n",
        "        predictions = self.fc(outputs).squeeze(0) #predictions = self.fc(outputs)\n",
        "\n",
        "        return predictions, hidden, cell, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3VqUkjVtNdE"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.decoder, self.encoder = decoder, encoder\n",
        "  \n",
        "    def forward(self, source, target, teacher_force_ratio = 0.5 ): #teacher_force_ratio hyper_parameter\n",
        "\n",
        "        target_len, target_vocab_size, batch_size = target.shape[0], num_decoder_tokens, source.shape[1]\n",
        "        encoder_states, hidden, cell = self.encoder(source)\n",
        "        # Grab the first input to the Decoder which will be <SOS> token\n",
        "        x = target[0]\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            # Use previous hidden, cell as context from encoder at start\n",
        "            output, hidden, cell, _ = self.decoder(x, encoder_states, hidden, cell)\n",
        "\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            best_guess, ind = output.argmax(1), t\n",
        "\n",
        "            # Store next output prediction\n",
        "            outputs[ind] = output\n",
        "\n",
        "            x = best_guess if random.random() >= teacher_force_ratio else  target[t]\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vGd1PyvtGRv"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        drop_par, emb_size, inp, fc_par = dropout, embedding_size, input_size, (hidden_size * 2)\n",
        "        self.dropout = nn.Dropout(drop_par)\n",
        "        self.num_layers, self.hidden_size, bidir = num_layers, hidden_size, True\n",
        "\n",
        "        self.embedding = nn.Embedding(inp, emb_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional = bidir)\n",
        "        \n",
        "        self.fc_hidden = nn.Linear(fc_par, hidden_size)\n",
        "        self.fc_cell = nn.Linear(fc_par, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (seq_length, N) where N is batch size\n",
        "        drop_par, fc_dim = self.embedding(x), 2\n",
        "        embedding = self.dropout(drop_par)\n",
        "        # embedding shape: (seq_length, N, embedding_size)\n",
        "        encoder_states, (hidden, cell) = self.rnn(embedding)\n",
        "        # outputs shape: (seq_length, N, hidden_size)\n",
        "        hidden_par = torch.cat((hidden[0:1], hidden[1:2]), dim = fc_dim)\n",
        "        cell_par = torch.cat((cell[0:1], cell[1:2]), dim = fc_dim)\n",
        "        hidden = self.fc_hidden(hidden_par)\n",
        "        cell = self.fc_cell(cell_par)\n",
        "\n",
        "        return encoder_states, hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RNN**"
      ],
      "metadata": {
        "id": "AjsKkEQgMVJQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jn-ebMgT2VPP"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        drop_par, emb_size = dropout, embedding_size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.num_layers, self.hidden_size, inp = num_layers, hidden_size, input_size\n",
        "        rnn_par =  hidden_size * 2 + emb_size\n",
        "        self.embedding = nn.Embedding(inp, emb_size)\n",
        "        rnn_lin_par, soft_dim = hidden_size * 3, 0\n",
        "        self.rnn = nn.RNN(rnn_par, hidden_size, num_layers)\n",
        "        self.energy = nn.Linear(rnn_lin_par, 1)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.Softmax(dim = soft_dim)\n",
        "        self.relu = nn.Hardshrink()\n",
        "\n",
        "    def forward(self, x, encoder_states, hidden):\n",
        "        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
        "        # is 1 here because we are sending in a single word and not a sentence\n",
        "        x, relu_dim = x.unsqueeze(0), 2\n",
        "        sequence_length, drop_par = encoder_states.shape[0], self.embedding(x)\n",
        "        embedding = self.dropout(drop_par)\n",
        "        # embedding shape: (1, N, embedding_size)\n",
        "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
        "        # h_reshaped: (seq_length, N, hidden_size*2)\n",
        "        relu_par = torch.cat((h_reshaped, encoder_states), dim = relu_dim)\n",
        "        energy = self.relu(self.energy(relu_par))\n",
        "        # energy: (seq_length, N, 1)\n",
        "        \n",
        "        attention = self.softmax(energy)\n",
        "        # attention: (seq_length, N, 1)\n",
        "        encoder_states = encoder_states.permute(1,0,2)\n",
        "\n",
        "        attention = attention.permute(1,2,0)\n",
        "        # attention: (N, seq_length, 1)\n",
        "\n",
        "        encoder_states = self.relu(encoder_states)\n",
        "        # encoder_states: (N, seq_length, hidden_size*2)\n",
        "        context_init = torch.bmm(attention, encoder_states)\n",
        "        context_vector = context_init.permute(1,0,2)\n",
        "        \n",
        "        rnn_input = torch.cat((context_vector, self.dropout(drop_par)),dim = relu_dim)\n",
        "       # rnn_input = self.relu(rnn_input)\n",
        "        outputs, (hidden) = self.rnn(rnn_input, (hidden))\n",
        "        # outputs shape: (1, N, hidden_size)\n",
        "\n",
        "        #predictions = self.fc(outputs)\n",
        "\n",
        "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
        "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
        "        # just gonna remove the first dim\n",
        "        predictions = self.fc(outputs).squeeze(0)\n",
        "\n",
        "        return predictions, hidden, attention\n",
        "class Seq2SeqGR(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2SeqGR, self).__init__()\n",
        "        self.decoder, self.encoder = decoder, encoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "        target_len, target_vocab_size, batch_size = target.shape[0], num_decoder_tokens, source.shape[1]\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "        \n",
        "        # Grab the first input to the Decoder which will be <SOS> token\n",
        "        x , n_len = target[0], target_len\n",
        "        encoder_states, hidden = self.encoder(source)\n",
        "\n",
        "        for t in range(1, n_len):\n",
        "            # Use previous hidden, cell as context from encoder at start\n",
        "            \n",
        "            output, hidden, _ = self.decoder(x, encoder_states, hidden)\n",
        "\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            ind, best_guess = t, output.argmax(1)\n",
        "\n",
        "            # Store next output prediction\n",
        "            outputs[ind] = output\n",
        "\n",
        "            x = best_guess if random.random() >= teacher_force_ratio else target[t]\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        drop_par, emb_size = dropout, embedding_size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.num_layers, self.hidden_size, inp, bid_dec = num_layers, hidden_size, input_size, True\n",
        "        self.embedding = nn.Embedding(input_size, emb_size)\n",
        "        fc_lin_par = hidden_size * 2\n",
        "        self.rnn = nn.RNN(emb_size, hidden_size, num_layers, bidirectional = bid_dec)\n",
        "        self.fc_hidden = nn.Linear(fc_lin_par, hidden_size)\n",
        "        self.fc_cell = nn.Linear(fc_lin_par, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (seq_length, N) where N is batch size\n",
        "        drop_par = self.embedding(x)\n",
        "        embedding = self.dropout(drop_par)\n",
        "        # embedding shape: (seq_length, N, embedding_size)\n",
        "\n",
        "        encoder_states, hidden = self.rnn(embedding)\n",
        "        # outputs shape: (seq_length, N, hidden_size)\n",
        "        fc_hidden_par, fc_dim = (hidden[0:1], hidden[1:2]), 2\n",
        "        hidden = self.fc_hidden(torch.cat(fc_hidden_par, dim = fc_dim))\n",
        "        #cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
        "\n",
        "        return encoder_states, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRU**"
      ],
      "metadata": {
        "id": "QXE3gqJ8MA0R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmW9_BRu839_"
      },
      "outputs": [],
      "source": [
        "class DecoderGRU(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(DecoderGRU, self).__init__()\n",
        "        drop_par, emb_size = dropout, embedding_size\n",
        "        self.dropout = nn.Dropout(drop_par)\n",
        "        self.num_layers, self.hidden_size, inp = num_layers, hidden_size, input_size\n",
        "        gru_par = hidden_size * 2 + emb_size\n",
        "        self.embedding = nn.Embedding(inp, emb_size)\n",
        "        self.rnn = nn.GRU(gru_par, hidden_size, num_layers)\n",
        "        lin_par, s_dim = hidden_size * 3, 0\n",
        "        self.energy = nn.Linear(lin_par, 1)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.Softmax(dim = s_dim)\n",
        "        self.relu = nn.Hardshrink()\n",
        "\n",
        "    def forward(self, x, encoder_states, hidden):\n",
        "        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
        "        # is 1 here because we are sending in a single word and not a sentence\n",
        "        x = x.unsqueeze(0)\n",
        "        drop_par, sequence_length = self.embedding(x), encoder_states.shape[0]\n",
        "        r_dim = 2\n",
        "        embedding = self.dropout(drop_par)\n",
        "        # embedding shape: (1, N, embedding_size)\n",
        "        \n",
        "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
        "        # h_reshaped: (seq_length, N, hidden_size*2)\n",
        "        relu_par = torch.cat((h_reshaped, encoder_states), dim = r_dim)\n",
        "        energy = self.relu(self.energy(relu_par))\n",
        "        # energy: (seq_length, N, 1)\n",
        "        \n",
        "        attention = self.softmax(energy)\n",
        "        # attention: (seq_length, N, 1)\n",
        "        encoder_states = encoder_states.permute(1,0,2)\n",
        "\n",
        "        attention = attention.permute(1,2,0)\n",
        "        # attention: (N, seq_length, 1)\n",
        "        encoder_states = self.relu(encoder_states)\n",
        "        # encoder_states: (N, seq_length, hidden_size*2)\n",
        "        context_init = torch.bmm(attention, encoder_states)\n",
        "        context_vector = context_init.permute(1,0,2)\n",
        "        \n",
        "        rnn_input = torch.cat((context_vector, self.dropout(drop_par)),dim = r_dim)\n",
        "       # rnn_input = self.relu(rnn_input)\n",
        "        outputs, (hidden) = self.rnn(rnn_input, (hidden))\n",
        "        # outputs shape: (1, N, hidden_size)\n",
        "\n",
        "        #predictions = self.fc(outputs)\n",
        "\n",
        "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
        "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
        "        # just gonna remove the first dim\n",
        "        predictions = self.fc(outputs).squeeze(0)\n",
        "\n",
        "        return predictions, hidden, attention\n",
        "class Seq2SeqGR(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2SeqGR, self).__init__()\n",
        "        self.decoder, self.encoder = decoder, encoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "        target_len, target_vocab_size, batch_size = target.shape[0], num_decoder_tokens, source.shape[1]\n",
        "        \n",
        "        encoder_states, hidden = self.encoder(source)\n",
        "\n",
        "         # Grab the first input to the Decoder which will be <SOS> token\n",
        "        n_len, x = target.shape[0], target[0]\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "        for t in range(1, n_len):\n",
        "            # Use previous hidden, cell as context from encoder at start\n",
        "            output, hidden, _ = self.decoder(x, encoder_states, hidden)\n",
        "\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            best_guess, ind = output.argmax(1), t\n",
        "\n",
        "            # Store next output prediction\n",
        "            outputs[ind] = output\n",
        "\n",
        "            x = best_guess if random.random() >= teacher_force_ratio else target[t]\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class EncoderGRU(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
        "        super(EncoderGRU, self).__init__()\n",
        "        drop_par, emb_size, bidir = dropout, embedding_size, True\n",
        "        self.dropout = nn.Dropout(drop_par)\n",
        "        self.num_layers, self.hidden_size, inp = num_layers, hidden_size, input_size\n",
        "        self.embedding = nn.Embedding(inp, emb_size)\n",
        "        fc_lin_par = hidden_size * 2\n",
        "        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, bidirectional = bidir)\n",
        "        \n",
        "        self.fc_hidden = nn.Linear(fc_lin_par, hidden_size)\n",
        "        self.fc_cell = nn.Linear(fc_lin_par, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (seq_length, N) where N is batch size\n",
        "        drop_par, fc_dim  = self.embedding(x), 2\n",
        "        embedding = self.dropout(drop_par)\n",
        "        # embedding shape: (seq_length, N, embedding_size)\n",
        "        \n",
        "        encoder_states, hidden = self.rnn(embedding)\n",
        "        # outputs shape: (seq_length, N, hidden_size)\n",
        "        fc_hidden_par = (hidden[0:1], hidden[1:2])\n",
        "        fc_par = torch.cat(fc_hidden_par, dim = fc_dim)\n",
        "        hidden = self.fc_hidden(fc_par)\n",
        "        #cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
        "\n",
        "        return encoder_states, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbayfjm-tX-_"
      },
      "outputs": [],
      "source": [
        "hidden_size = 128  # Needs to be the same for both RNN's\n",
        "dec_dropout, enc_dropout = 0.8, 0.8\n",
        "decoder_embedding_size, encoder_embedding_size = 128, 128\n",
        "num_layers, output_size = 1, num_decoder_tokens\n",
        "input_size_decoder, input_size_encoder = num_decoder_tokens, num_encoder_tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_searchGR(model, word, input_char_index, output_char_index, reverse_input_char_index,\n",
        "                reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length,\n",
        "                num_encoder_tokens, num_decoder_tokens, beam_width, device, length_penalty=0.6):\n",
        "\n",
        "    # Encode the input word\n",
        "    data, word_t = np.zeros((max_encoder_seq_length, 1), dtype= d_type), ''\n",
        "    t_z, tab_pad, nl_pad,prob_dim, bw = 0, '\\t', '\\n', 1, beam_width\n",
        "    for t, char in enumerate(word):\n",
        "        data[t, 0] = input_char_index[char]\n",
        "    t_z = t+1\n",
        "    data[t_z :, 0] = input_char_index[c_end]\n",
        "\n",
        "    data = torch.tensor(data, dtype= td_type).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_states, hidden = model.encoder(data)\n",
        "\n",
        "    # Initialize beam\n",
        "    out_t = output_char_index[tab_pad]\n",
        "    out_reshape = np.array(out_t).reshape(1,)\n",
        "    hidden_par = hidden.unsqueeze(0)\n",
        "    initial_sequence = torch.tensor(out_reshape).to(device)\n",
        "    beam = [(0.0, initial_sequence, hidden_par)]  # [(score, sequence, hidden)]\n",
        "\n",
        "    for _ in range(max_decoder_seq_length):\n",
        "      score_cal , candidates = 0, []\n",
        "      for score, seq, hidden in beam:\n",
        "          score_cal = score_cal + score\n",
        "          last_token = seq[-1].item()\n",
        "          if last_token == output_char_index[nl_pad]:\n",
        "              # If the sequence ends with the end token, add it to the candidates\n",
        "              candidates.append((score, seq, hidden))\n",
        "              \n",
        "              continue\n",
        "          \n",
        "          last_t_reshape = np.array(last_token).reshape(1,)\n",
        "          hidden_par = hidden.squeeze(0)\n",
        "          x = torch.tensor(last_t_reshape).to(device)\n",
        "          output, hidden,at = model.decoder(x,encoder_states, hidden_par)\n",
        "          probabilities = F.softmax(output, dim = prob_dim)\n",
        "\n",
        "          # Get the top-k probabilities and tokens\n",
        "          topk_probs, topk_tokens = torch.topk(probabilities, k= bw)\n",
        "          cal_score = score\n",
        "          for prob, token in zip(topk_probs[0], topk_tokens[0]):\n",
        "              cal_score = cal_score + prob\n",
        "              new_seq = torch.cat((seq, token.unsqueeze(0)), dim=0)\n",
        "              n_hidden = hidden.clone()\n",
        "              new_seq_ln  = len(new_seq)\n",
        "              pen_fac = (new_seq_ln - 1) / 5\n",
        "              new_hidden = n_hidden.unsqueeze(0)\n",
        "              #length_penalty_factor = ((pen_fac) ** length_penalty)  # Adjust penalty factor as needed\n",
        "              \n",
        "              candidates.append((score + torch.log(prob).item() / ((pen_fac) ** length_penalty), new_seq, new_hidden))\n",
        "              cal_score = 0\n",
        "      # Select top-k candidates based on the accumulated scores\n",
        "      beam = heapq.nlargest(beam_width, candidates, key = lambda x: x[0])\n",
        "\n",
        "    \n",
        "    best_score, best_sequence, _ = max(beam, key=lambda x: x[0])# Select the best sequence from the beam as the output\n",
        "    \n",
        "    word_t = ''.join([reverse_target_char_index[token.item()] for token in best_sequence[1:-1]])\n",
        "\n",
        "    return word_t"
      ],
      "metadata": {
        "id": "0z8lxErILuYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mE8EkzIitSrc"
      },
      "outputs": [],
      "source": [
        "\n",
        "def beam_search(model, word, input_char_index, output_char_index, reverse_input_char_index,reverse_target_char_index, max_encoder_seq_length, \n",
        "                max_decoder_seq_length,num_encoder_tokens, num_decoder_tokens, beam_width, device, length_penalty=0.6):\n",
        "\n",
        "\n",
        "    # Encode the input word\n",
        "    data, word_t = np.zeros((max_encoder_seq_length, 1), dtype= d_type), ''\n",
        "    t_z , bw = 0, beam_width\n",
        "    for t, char in enumerate(word):\n",
        "        data[t, 0] = input_char_index[char]\n",
        "    t_z = t+1\n",
        "    data[t_z:, 0] = input_char_index[c_end]\n",
        "\n",
        "    data = torch.tensor(data, dtype= td_type ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_states,hidden,cell = model.encoder(data)\n",
        "\n",
        "    # Initialize beam\n",
        "    out_t = output_char_index['\\t']\n",
        "    out_reshape = np.array(out_t).reshape(1,)\n",
        "    hidden_par = hidden.unsqueeze(0)\n",
        "    initial_sequence = torch.tensor(out_reshape).to(device)\n",
        "    beam = [(0.0, initial_sequence, hidden_par)]  # [(score, sequence, hidden)]\n",
        "\n",
        "    for _ in range(max_decoder_seq_length):\n",
        "      candidates = []\n",
        "      for score, seq, hidden in beam:\n",
        "          # last_token = seq[-1].item()\n",
        "          if seq[-1].item() == output_char_index['\\n']:\n",
        "              # If the sequence ends with the end token, add it to the candidates\n",
        "              candidates.append((score, seq, hidden))\n",
        "              continue\n",
        "          lt_reshape = np.array(seq[-1].item()).reshape(1,) # last_token = seq[-1].item()\n",
        "          hidden_upar = hidden.squeeze(0)\n",
        "          x = torch.tensor(lt_reshape).to(device)\n",
        "          output, hidden,cell,at = model.decoder(x, encoder_states, hidden_upar,cell)\n",
        "          probabilities = F.softmax(output, dim=1)\n",
        "          cal_cand = 0\n",
        "          # Get the top-k probabilities and tokens\n",
        "          topk_probs, topk_tokens = torch.topk(probabilities, k=bw)\n",
        "\n",
        "          for prob, token in zip(topk_probs[0], topk_tokens[0]):\n",
        "              cal_cand = prob\n",
        "              n_hidden = hidden.clone()\n",
        "              new_seq = torch.cat((seq, token.unsqueeze(0)), dim=0)\n",
        "              ln_ns = len(new_seq)\n",
        "              ln_pf = ((ln_ns - 1) / 5)\n",
        "              new_hidden = n_hidden.unsqueeze(0)\n",
        "              #length_penalty_factor = ln_pf ** length_penalty  # Adjust penalty factor as needed\n",
        "              candidate_cal = score + torch.log(prob).item() / (ln_pf ** length_penalty)\n",
        "              candidates.append((candidate_cal, new_seq, new_hidden))\n",
        "\n",
        "      beam = heapq.nlargest(bw, candidates, key=lambda x: x[0])# Select top-k candidates based on the accumulated scores\n",
        "\n",
        "      \n",
        "    best_score, best_sequence, _ = max(beam, key=lambda x: x[0]) # Select the best sequence from the beam as the output\n",
        "    \n",
        "    cal_score = best_score\n",
        "    word_t = ''.join([reverse_target_char_index[token.item()] for token in best_sequence[1:-1]])\n",
        "    cal_score = 0\n",
        "    return word_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSSEebvWwisM"
      },
      "outputs": [],
      "source": [
        "hidden_size = 256  # Needs to be the same for both RNN's\n",
        "load_model, training = False, False\n",
        "output_size = num_decoder_tokens\n",
        "num_epochs, batch_size, learning_rate = 2, 32, 0.001\n",
        "dec_dropout, enc_dropout = 0.1, 0.1\n",
        "encoder_embedding_size, decoder_embedding_size = 256, 256\n",
        "num_dec_layers,num_enc_layers = 1, 1\n",
        "input_size_encoder, input_size_decoder = num_encoder_tokens, num_decoder_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sjm6SLwJB9xr"
      },
      "outputs": [],
      "source": [
        "def training(num_encoder_tokens,input_embedding_size, dp, cell_type, hidden_size, num_enc_layers, num_dec_layers,num_epochs,output_size,input_size_decoder,batch_size,beam_width):\n",
        "    #Decoder Selection\n",
        "    if(cell_type==\"LSTM\"):\n",
        "        decoder_net = Decoder(input_size_decoder,input_embedding_size,hidden_size,output_size,num_dec_layers,dp).to(device)\n",
        "    elif(cell_type==\"GRU\"):\n",
        "        decoder_net = DecoderGRU(input_size_decoder,decoder_embedding_size,hidden_size,output_size,num_dec_layers,dec_dropout).to(device)\n",
        "    else:\n",
        "        decoder_net = DecoderRNN(input_size_decoder,decoder_embedding_size,hidden_size,output_size,num_dec_layers,dec_dropout).to(device)\n",
        "    #Encoder Selection\n",
        "    if(cell_type==\"LSTM\"):\n",
        "        encoder_net = Encoder(input_size_encoder,input_embedding_size, hidden_size, num_enc_layers,dp).to(device)\n",
        "    elif(cell_type==\"GRU\"):\n",
        "        encoder_net = EncoderGRU(input_size_encoder, encoder_embedding_size, hidden_size, num_enc_layers, enc_dropout).to(device)\n",
        "    else:\n",
        "        encoder_net = EncoderRNN(input_size_encoder, encoder_embedding_size, hidden_size, num_enc_layers, enc_dropout).to(device)\n",
        "    \n",
        "    #Model Selection\n",
        "    if(cell_type==\"LSTM\"):    \n",
        "        model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
        "    else:\n",
        "        model = Seq2SeqGR(encoder_net, decoder_net).to(device)\n",
        "    \n",
        "    split_dim, bs,  m_norm = 1, batch_size, 1\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    train_ds_y, train_ds_x = torch.split(target_data, bs, dim = split_dim), torch.split(input_data, bs, dim = split_dim)\n",
        "    correct_prediction  = 0\n",
        "    #print(train_ds_x)\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "        model.eval()\n",
        "        total_count = len(val_X)\n",
        "        model.train()\n",
        "        for i, (x,y) in enumerate(zip(train_ds_x,train_ds_y)):\n",
        "            # Get input and targets and get to cuda\n",
        "            target, inp_data = y.to(device), x.to(device)\n",
        "            # Forward prop\n",
        "            output = model(inp_data, target)\n",
        "            target, output = target[1:].reshape(-1),  output[1:].reshape(-1, output.shape[2])\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward() # Back prop\n",
        "            # Clip to avoid exploding gradient issues, makes sure grads are\n",
        "            # within a healthy range\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = m_norm)\n",
        "            # Gradient descent step\n",
        "            optimizer.step()\n",
        "        correct_pred, total_words, lstm_bw = 0, total_count, 1\n",
        "        model.eval()\n",
        "        for i in range(total_count):\n",
        "            if(cell_type==\"LSTM\"):\n",
        "                decoded_sentence = beam_search(model,val_X[i], input_char_index, output_char_index, reverse_input_char_index, reverse_target_char_index, \n",
        "                                                     max_encoder_seq_length, max_decoder_seq_length,num_encoder_tokens, num_decoder_tokens,lstm_bw,device)\n",
        "            else:\n",
        "                 decoded_sentence = beam_searchGR(model,val_X[i], input_char_index, output_char_index, reverse_input_char_index, reverse_target_char_index, \n",
        "                                                        max_encoder_seq_length, max_decoder_seq_length,num_encoder_tokens, num_decoder_tokens,beam_width,device)\n",
        "                 \n",
        "            if decoded_sentence == val_Y[i][1:-1]:\n",
        "                correct_pred = correct_pred + 1\n",
        "        test_accuracy = correct_pred / total_words\n",
        "        print(correct_pred / total_words)  # test_accuracy = correct_pred / total_words\n",
        "        wandb.log({'val_accuracy_with_attention' : test_accuracy*100})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGMv-brttIAa"
      },
      "outputs": [],
      "source": [
        "sweep_config2 = {\n",
        "    'method': 'bayes',\n",
        "    'parameters': {\n",
        "                    'Number_Epochs_at' :{\n",
        "                      'values':[10,20,30,40]\n",
        "                      },\n",
        "                    'cell_type_value_at':{\n",
        "                      'values': ['LSTM','GRU','RNN']\n",
        "                      },\n",
        "                    'beam_width_at':{\n",
        "                      'values':[1,2,3,4,5]\n",
        "                      },\n",
        "                   'hidden_size_EncDec_at': {\n",
        "                      'values': [128, 256, 512]\n",
        "                      },\n",
        "                   'dropout_EncDec_at': {\n",
        "                      'values': [0.1, 0.2, 0.3, 0.4]\n",
        "                      },\n",
        "                   'embedding_size_input_at': {\n",
        "                      'values': [128, 256, 512]\n",
        "                      },\n",
        "                   'beam_width_at':{\n",
        "                      'values':[1,2,3,4,5]\n",
        "                      },\n",
        "                   'batch_size_value_at': {\n",
        "                      'values': [128,256,512]\n",
        "                      }\n",
        "                  },\n",
        "     'metric': {'goal': 'maximize', 'name': 'val_accuracy_with_attention'}           \n",
        " }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLHZ-RgIHP-z"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    var1 = wandb.init()\n",
        "    var2 = var1.config\n",
        "     # var2 is a variable that holds and saves hyperparameters and inputs\n",
        "    wandb.run.name = 'Cell_Type:-' + var2.cell_type_value_at + ', Batch_Size:-' + str(var2.batch_size_value_at) + ', Epochs:-' + str(var2.Number_Epochs_at) + ', Dropout:-' + str(var2.dropout_EncDec_at) + ', Beam_Search:-' + str(var2.beam_width_at) +', Embedding_Size:-' + str(var2.embedding_size_input_at) + ', Hidden_Size:-' + str(var2.hidden_size_EncDec_at) \n",
        "    # training(input_size_encoder ,var2.embedding_size_input_at, var2.dropout_EncDec_at, 'LSTM', var2.hidden_size_EncDec_at, 1,  1,5,num_decoder_tokens,num_decoder_tokens,var2.batch_size_value_at,var2.beam_width_at)\n",
        "    training(input_size_encoder ,var2.embedding_size_input_at, var2.dropout_EncDec_at, var2.cell_type_value_at, var2.hidden_size_EncDec_at, 1,  1,var2.Number_Epochs_at,num_decoder_tokens,num_decoder_tokens,var2.batch_size_value_at,var2.beam_width_at)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8ed96a4d68b84a6e894504c6df216d99",
            "3f9df88fe2164b2395a55051869582f0",
            "eed3c86883fa4c9ca7a749b53abd2573",
            "d58ef3c05c9c4e00af752046c0063501",
            "51cc5c206f904aa0a5eb77e5a26fde13",
            "120fe4bbfa31476382066fb9cf1695f9",
            "1bc5c6bdb7c249cdbc307ed5c47979f7",
            "aac989cf6aa4467880e8bb183b68e951"
          ]
        },
        "id": "EnL8ajeFIL-w",
        "outputId": "935a4dd3-315b-46fe-e414-f4e34c6b64ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 2h1c2fdv\n",
            "Sweep URL: https://wandb.ai/cs22m083/Assignment3_Q5/sweeps/2h1c2fdv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7nfx1cig with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tNumber_Epochs_at: 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size_value_at: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width_at: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type_value_at: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_EncDec_at: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size_input_at: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size_EncDec_at: 256\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/aksharantar_dataset/tel/wandb/run-20230520_082329-7nfx1cig</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22m083/Assignment3_Q5/runs/7nfx1cig' target=\"_blank\">legendary-sweep-1</a></strong> to <a href='https://wandb.ai/cs22m083/Assignment3_Q5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m083/Assignment3_Q5/sweeps/2h1c2fdv' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_Q5/sweeps/2h1c2fdv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22m083/Assignment3_Q5' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_Q5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/cs22m083/Assignment3_Q5/sweeps/2h1c2fdv' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_Q5/sweeps/2h1c2fdv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22m083/Assignment3_Q5/runs/7nfx1cig' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_Q5/runs/7nfx1cig</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0 / 30]\n",
            "0.01611328125\n",
            "[Epoch 1 / 30]\n",
            "0.128173828125\n",
            "[Epoch 2 / 30]\n",
            "0.241943359375\n",
            "[Epoch 3 / 30]\n",
            "0.307373046875\n",
            "[Epoch 4 / 30]\n",
            "0.35546875\n",
            "[Epoch 5 / 30]\n",
            "0.38720703125\n",
            "[Epoch 6 / 30]\n",
            "0.400390625\n",
            "[Epoch 7 / 30]\n",
            "0.435302734375\n",
            "[Epoch 8 / 30]\n",
            "0.44140625\n",
            "[Epoch 9 / 30]\n",
            "0.429931640625\n",
            "[Epoch 10 / 30]\n",
            "0.459716796875\n",
            "[Epoch 11 / 30]\n",
            "0.47509765625\n",
            "[Epoch 12 / 30]\n",
            "0.48193359375\n",
            "[Epoch 13 / 30]\n",
            "0.490478515625\n",
            "[Epoch 14 / 30]\n",
            "0.48095703125\n",
            "[Epoch 15 / 30]\n",
            "0.481201171875\n",
            "[Epoch 16 / 30]\n",
            "0.501220703125\n",
            "[Epoch 17 / 30]\n",
            "0.49365234375\n",
            "[Epoch 18 / 30]\n",
            "0.49951171875\n",
            "[Epoch 19 / 30]\n",
            "0.507080078125\n",
            "[Epoch 20 / 30]\n",
            "0.504150390625\n",
            "[Epoch 21 / 30]\n",
            "0.51708984375\n",
            "[Epoch 22 / 30]\n",
            "0.4990234375\n",
            "[Epoch 23 / 30]\n",
            "0.501220703125\n",
            "[Epoch 24 / 30]\n",
            "0.502197265625\n",
            "[Epoch 25 / 30]\n",
            "0.508544921875\n",
            "[Epoch 26 / 30]\n",
            "0.513671875\n",
            "[Epoch 27 / 30]\n",
            "0.515625\n",
            "[Epoch 28 / 30]\n",
            "0.515869140625\n",
            "[Epoch 29 / 30]\n",
            "0.515625\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.010 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.109039…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ed96a4d68b84a6e894504c6df216d99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>val_accuracy_with_attention</td><td>▁▃▄▅▆▆▆▇▇▇▇▇██▇▇██████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>val_accuracy_with_attention</td><td>51.5625</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">legendary-sweep-1</strong> at: <a href='https://wandb.ai/cs22m083/Assignment3_Q5/runs/7nfx1cig' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_Q5/runs/7nfx1cig</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230520_082329-7nfx1cig/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xq0v2dvq with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tNumber_Epochs_at: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size_value_at: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width_at: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type_value_at: GRU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_EncDec_at: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size_input_at: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size_EncDec_at: 256\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/aksharantar_dataset/tel/wandb/run-20230520_085145-xq0v2dvq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22m083/Assignment3_Q5/runs/xq0v2dvq' target=\"_blank\">fast-sweep-2</a></strong> to <a href='https://wandb.ai/cs22m083/Assignment3_Q5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m083/Assignment3_Q5/sweeps/2h1c2fdv' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_Q5/sweeps/2h1c2fdv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22m083/Assignment3_Q5' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_Q5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/cs22m083/Assignment3_Q5/sweeps/2h1c2fdv' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_Q5/sweeps/2h1c2fdv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22m083/Assignment3_Q5/runs/xq0v2dvq' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_Q5/runs/xq0v2dvq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0 / 10]\n",
            "0.23828125\n",
            "[Epoch 1 / 10]\n",
            "0.35400390625\n",
            "[Epoch 2 / 10]\n",
            "0.40283203125\n",
            "[Epoch 3 / 10]\n",
            "0.427978515625\n",
            "[Epoch 4 / 10]\n",
            "0.451416015625\n",
            "[Epoch 5 / 10]\n",
            "0.4677734375\n",
            "[Epoch 6 / 10]\n"
          ]
        }
      ],
      "source": [
        "sweep_id = wandb.sweep(sweep_config2, project=\"Assignment3_final\")\n",
        "wandb.agent(sweep_id, train, count=25)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8ed96a4d68b84a6e894504c6df216d99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f9df88fe2164b2395a55051869582f0",
              "IPY_MODEL_eed3c86883fa4c9ca7a749b53abd2573"
            ],
            "layout": "IPY_MODEL_d58ef3c05c9c4e00af752046c0063501"
          }
        },
        "3f9df88fe2164b2395a55051869582f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51cc5c206f904aa0a5eb77e5a26fde13",
            "placeholder": "​",
            "style": "IPY_MODEL_120fe4bbfa31476382066fb9cf1695f9",
            "value": "0.001 MB of 0.010 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "eed3c86883fa4c9ca7a749b53abd2573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bc5c6bdb7c249cdbc307ed5c47979f7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aac989cf6aa4467880e8bb183b68e951",
            "value": 0.10903981264637003
          }
        },
        "d58ef3c05c9c4e00af752046c0063501": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51cc5c206f904aa0a5eb77e5a26fde13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "120fe4bbfa31476382066fb9cf1695f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bc5c6bdb7c249cdbc307ed5c47979f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aac989cf6aa4467880e8bb183b68e951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}