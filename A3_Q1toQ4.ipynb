{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3186a016a4484ef48d98b6ce4ac0d629": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a181a0ded8c4dd18c11acc55978662a",
              "IPY_MODEL_ae096a69596c4a05bed6da2d4be9975b"
            ],
            "layout": "IPY_MODEL_0a6b49e5b90d4f63b0b4043619f8ce95"
          }
        },
        "1a181a0ded8c4dd18c11acc55978662a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46775b5759d3456b9c342ed72e01e29c",
            "placeholder": "​",
            "style": "IPY_MODEL_556d3cab10464f70a428a04a60aeb889",
            "value": "0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "ae096a69596c4a05bed6da2d4be9975b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a13a586326a46e2b19f958bb9d8763a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81814895e5104850a50cbf8c5d6364c3",
            "value": 0.9872623396085043
          }
        },
        "0a6b49e5b90d4f63b0b4043619f8ce95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46775b5759d3456b9c342ed72e01e29c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "556d3cab10464f70a428a04a60aeb889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a13a586326a46e2b19f958bb9d8763a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81814895e5104850a50cbf8c5d6364c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d7c0b4063e84ce3a27ac4d719b7dd0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3af13a92456841e8954cb91e2e4e2b03",
              "IPY_MODEL_cb8ec0b65827474b814034eb559feb64"
            ],
            "layout": "IPY_MODEL_8a73cceda6d74324a52b397b010e227f"
          }
        },
        "3af13a92456841e8954cb91e2e4e2b03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49f3da8f202e40c884744b15cf6ed699",
            "placeholder": "​",
            "style": "IPY_MODEL_9bdc014e0fcc4fe7867fb4f5fa65f807",
            "value": "0.009 MB of 0.010 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "cb8ec0b65827474b814034eb559feb64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6001d6d6cf864fdf94ac52d129b036bf",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3a38cde893d47e5bc53ee68dde92fa3",
            "value": 0.9864824570122255
          }
        },
        "8a73cceda6d74324a52b397b010e227f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49f3da8f202e40c884744b15cf6ed699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bdc014e0fcc4fe7867fb4f5fa65f807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6001d6d6cf864fdf94ac52d129b036bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3a38cde893d47e5bc53ee68dde92fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import csv\n",
        "!pip install wandb\n",
        "import wandb\n",
        "import heapq\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.ticker as ticker\n",
        "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:18.051562Z",
          "iopub.execute_input": "2023-05-17T12:52:18.052003Z",
          "iopub.status.idle": "2023-05-17T12:52:30.712510Z",
          "shell.execute_reply.started": "2023-05-17T12:52:18.051962Z",
          "shell.execute_reply": "2023-05-17T12:52:30.711467Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfvFRD6VkWDP",
        "outputId": "90a5a34d-fd5b-4aed-eaf9-6a9c0e3a6850"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.23.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key='b032cc059132c9aac4f1b317f6f9ad007ef9e4d4')\n",
        "wandb.init(project=\"Assignment3_ltry\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "zFb-7o4FJTsi",
        "outputId": "e5241703-f9cd-4aad-c4ed-4b6af9181d4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs22m083\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230518_092341-69lj20tj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22m083/Assignment3_ltry/runs/69lj20tj' target=\"_blank\">grateful-microwave-8</a></strong> to <a href='https://wandb.ai/cs22m083/Assignment3_ltry' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22m083/Assignment3_ltry' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_ltry</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22m083/Assignment3_ltry/runs/69lj20tj' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_ltry/runs/69lj20tj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cs22m083/Assignment3_ltry/runs/69lj20tj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7efd5249f730>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE6g7bby0rZh",
        "outputId": "8da173d1-2440-4514-f227-d4e005673da8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/aksharantar_dataset/tel/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra6h-RxQ0vVH",
        "outputId": "d36f13e0-ca3d-4012-daf1-6a1e519c03f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/aksharantar_dataset/tel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tsv_file = open(\"/content/drive/MyDrive/aksharantar_dataset/tel/tel_train.csv\")\n",
        "test_tsv_file = open(\"/content/drive/MyDrive/aksharantar_dataset/tel/tel_test.csv\")\n",
        "val_tsv_file = open(\"/content/drive/MyDrive/aksharantar_dataset/tel/tel_valid.csv\")\n",
        "read_tsv = csv.reader(tsv_file)\n",
        "test_read_tsv = csv.reader(test_tsv_file)\n",
        "val_read_tsv = csv.reader(val_tsv_file)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:30.722634Z",
          "iopub.execute_input": "2023-05-17T12:52:30.728581Z",
          "iopub.status.idle": "2023-05-17T12:52:30.748886Z",
          "shell.execute_reply.started": "2023-05-17T12:52:30.728541Z",
          "shell.execute_reply": "2023-05-17T12:52:30.747588Z"
        },
        "trusted": true,
        "id": "RCvT2zejkWDX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, train_Y = [], []\n",
        "for i in read_tsv:\n",
        "    train_X.append(i[0])\n",
        "    train_Y.append(i[1])\n",
        "    \n",
        "test_X,test_Y = [], []\n",
        "for i in test_read_tsv:\n",
        "    test_X.append(i[0])\n",
        "    test_Y.append(i[1])\n",
        "    \n",
        "val_X, val_Y = [], []\n",
        "for i in val_read_tsv:\n",
        "    val_X.append(i[0])\n",
        "    val_Y.append(i[1])\n",
        "    \n",
        "\n",
        "\n",
        "test_X, test_Y = np.array(test_X), np.array(test_Y)\n",
        "t_range = test_Y.shape[0]\n",
        "val_X, val_Y = np.array(val_X),  np.array(val_Y)\n",
        "v_range = val_Y.shape[0]\n",
        "train_X, train_Y = np.array(train_X), np.array(train_Y)\n",
        "tr_range = train_Y.shape[0]\n",
        "for i in range(t_range):\n",
        "    test_Y[i] = \"\\t\" + test_Y[i] + \"\\n\"\n",
        "    \n",
        "for i in range(v_range):\n",
        "    val_Y[i] = \"\\t\" + val_Y[i] + \"\\n\"\n",
        "\n",
        "for i in range(tr_range):\n",
        "    train_Y[i] = \"\\t\" + train_Y[i] + \"\\n\"\n",
        "ch_end = \" \""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:30.750334Z",
          "iopub.execute_input": "2023-05-17T12:52:30.750654Z",
          "iopub.status.idle": "2023-05-17T12:52:31.106901Z",
          "shell.execute_reply.started": "2023-05-17T12:52:30.750622Z",
          "shell.execute_reply": "2023-05-17T12:52:31.105912Z"
        },
        "trusted": true,
        "id": "ScpIcwmtkWDY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_input_corpus, output_corpus, input_corpus,  val_output_corpus = set(), set(), set(), set()\n",
        "\n",
        "for word in train_Y:\n",
        "    for char in word:\n",
        "        if char not in output_corpus:\n",
        "            output_corpus.add(char)\n",
        "\n",
        "for word in train_X:\n",
        "    for char in word:\n",
        "        if char not in input_corpus:\n",
        "            input_corpus.add(char)\n",
        "\n",
        "output_corpus.add(ch_end)\n",
        "input_corpus.add(ch_end)\n",
        "\n",
        "\n",
        "for word in val_Y:\n",
        "    for char in word:\n",
        "        if char not in val_output_corpus:\n",
        "            val_output_corpus.add(char)\n",
        "\n",
        "output_corpus,  input_corpus = sorted(list(output_corpus)), sorted(list(input_corpus))\n",
        "num_encoder_tokens =  len(input_corpus)  \n",
        "\n",
        "for word in val_X:\n",
        "    for char in word:\n",
        "        if char not in val_input_corpus:\n",
        "            val_input_corpus.add(char)\n",
        "\n",
        "num_decoder_tokens = len(output_corpus)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:31.113255Z",
          "iopub.execute_input": "2023-05-17T12:52:31.115702Z",
          "iopub.status.idle": "2023-05-17T12:52:31.502261Z",
          "shell.execute_reply.started": "2023-05-17T12:52:31.115664Z",
          "shell.execute_reply": "2023-05-17T12:52:31.501219Z"
        },
        "trusted": true,
        "id": "o6M3XpFNkWDZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_decoder_seq_length,max_encoder_seq_length = max([len(txt) for txt in train_Y]), max([len(txt) for txt in train_X]) + 2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:31.503520Z",
          "iopub.execute_input": "2023-05-17T12:52:31.504076Z",
          "iopub.status.idle": "2023-05-17T12:52:31.610035Z",
          "shell.execute_reply.started": "2023-05-17T12:52:31.504028Z",
          "shell.execute_reply": "2023-05-17T12:52:31.609121Z"
        },
        "trusted": true,
        "id": "7xXKymJfkWDa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "print(\"Number of samples:\", len(train_X))\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:31.611278Z",
          "iopub.execute_input": "2023-05-17T12:52:31.611856Z",
          "iopub.status.idle": "2023-05-17T12:52:31.619583Z",
          "shell.execute_reply.started": "2023-05-17T12:52:31.611823Z",
          "shell.execute_reply": "2023-05-17T12:52:31.618578Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAweNDwakWDa",
        "outputId": "8602ec62-a8a4-441e-eb98-7aeb49134eb8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique input tokens: 27\n",
            "Max sequence length for outputs: 21\n",
            "Number of samples: 51200\n",
            "Max sequence length for inputs: 30\n",
            "Number of unique output tokens: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, num_epochs, learning_rate = 32, 10, 0.001\n",
        "load_model, training = False, False\n",
        "input_size_encoder, input_size_decoder = num_encoder_tokens, num_decoder_tokens\n",
        "output_size = num_decoder_tokens\n",
        "encoder_embedding_size, decoder_embedding_size = 256, 256\n",
        "hidden_size = 256  # Needs to be the same for both RNN's\n",
        "num_enc_layers, num_dec_layers = 1, 1\n",
        "enc_dropout, dec_dropout = 0.1, 0.1\n",
        "d_type, td_type = \"int64\", torch.int64"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:31.620774Z",
          "iopub.execute_input": "2023-05-17T12:52:31.621710Z",
          "iopub.status.idle": "2023-05-17T12:52:31.631916Z",
          "shell.execute_reply.started": "2023-05-17T12:52:31.621677Z",
          "shell.execute_reply": "2023-05-17T12:52:31.630727Z"
        },
        "trusted": true,
        "id": "utKX4bqkkWDc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ln = len(train_X)\n",
        "val_ln = len(val_X)\n",
        "input_char_index, output_char_index = dict([(char, i) for i, char in enumerate(input_corpus)]), dict([(char, i) for i, char in enumerate(output_corpus)])\n",
        "input_data, target_data = np.zeros((max_encoder_seq_length,train_ln), dtype= d_type), np.zeros((max_decoder_seq_length,train_ln), dtype=d_type)\n",
        "c_end = \" \"\n",
        "input_data_val, target_data_val = np.zeros((max_encoder_seq_length,val_ln), dtype=d_type), np.zeros((max_decoder_seq_length,val_ln), dtype=d_type)\n",
        "for i, (x, y) in enumerate(zip(val_X, val_Y)):\n",
        "    t_n = i+ val_ln\n",
        "    for t, char in enumerate(y):\n",
        "        target_data_val[t, i] = output_char_index[char]\n",
        "    t_n = t+1       \n",
        "    target_data_val[t_n :,i] = output_char_index[c_end]\n",
        "    t_n = 0\n",
        "    for t, char in enumerate(x):\n",
        "        input_data_val[t, i] = input_char_index[char]\n",
        "    t_n = t+1   \n",
        "    input_data_val[t_n :,i] = input_char_index[c_end] \n",
        "t_n =0    \n",
        "for i, (x, y) in enumerate(zip(train_X, train_Y)):\n",
        "    t_n = i+ val_ln\n",
        "    for t, char in enumerate(y):\n",
        "        target_data[t, i] = output_char_index[char]\n",
        "    t_n  = t+1        \n",
        "    target_data[t_n :,i] = output_char_index[c_end]\n",
        "    t_n =0\n",
        "    for t, char in enumerate(x):\n",
        "        input_data[t, i] = input_char_index[char]\n",
        "    t_n = t+1   \n",
        "    input_data[t_n :,i] = input_char_index[c_end]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:31.633411Z",
          "iopub.execute_input": "2023-05-17T12:52:31.633982Z",
          "iopub.status.idle": "2023-05-17T12:52:33.170614Z",
          "shell.execute_reply.started": "2023-05-17T12:52:31.633949Z",
          "shell.execute_reply": "2023-05-17T12:52:33.169597Z"
        },
        "trusted": true,
        "id": "BafE7JfVkWDc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convertin numpy arrays to tensors\n",
        "input_data_val, target_data_val = torch.tensor(input_data_val,dtype = td_type), torch.tensor(target_data_val,dtype = td_type)\n",
        "input_data, target_data = torch.tensor(input_data,dtype = td_type), torch.tensor(target_data,dtype = td_type)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.171878Z",
          "iopub.execute_input": "2023-05-17T12:52:33.172542Z",
          "iopub.status.idle": "2023-05-17T12:52:33.235548Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.172505Z",
          "shell.execute_reply": "2023-05-17T12:52:33.234579Z"
        },
        "trusted": true,
        "id": "ofDaFzv0kWDd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_target_char_index,reverse_input_char_index, t_z = dict((i, char) for char, i in output_char_index.items()), dict((i, char) for char, i in input_char_index.items()), 0"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.239789Z",
          "iopub.execute_input": "2023-05-17T12:52:33.240674Z",
          "iopub.status.idle": "2023-05-17T12:52:33.250214Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.240640Z",
          "shell.execute_reply": "2023-05-17T12:52:33.249108Z"
        },
        "trusted": true,
        "id": "Sf0BRbl0kWDe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM** \n"
      ],
      "metadata": {
        "id": "mtcIisUPs_8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module): \n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        drop_par = dropout\n",
        "        self.dropout = nn.Dropout(drop_par)\n",
        "        self.num_layers, self.hidden_size, = num_layers, hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n",
        "\n",
        "    def forward(self, x): # x shape: (seq_length, N) where N is batch size\n",
        "        drop_par = self.embedding(x)\n",
        "        embedding = self.dropout(drop_par) # embedding shape: (seq_length, N, embedding_size)\n",
        "        outputs, (hidden, cell) = self.rnn(self.dropout(drop_par)) # outputs shape: (seq_length, N, hidden_size)\n",
        "        return hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder):\n",
        "\n",
        "        super(Seq2Seq, self).__init__()  \n",
        "        self.decoder, self.encoder = decoder, encoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.2):\n",
        "        batch_size, target_len, target_vocab_size = source.shape[1], target.shape[0], num_decoder_tokens\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "        # Grab the first input to the Decoder which will be <SOS> token\n",
        "        x = target[0]\n",
        "        hidden, cell = self.encoder(source)\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            # Use previous hidden, cell as context from encoder at start\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "\n",
        "            # Store next output prediction\n",
        "            outputs[t], best_guess = output, output.argmax(1)\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            x = best_guess if random.random() >= teacher_force_ratio else target[t]\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        drop_par, hidden_layer_size = dropout, hidden_size\n",
        "        self.dropout,self.num_layers, self.hidden_size = nn.Dropout(drop_par),  num_layers, hidden_layer_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout= drop_par)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell): # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
        "        # is 1 here because we are sending in a single word and not a sentence\n",
        "        x  = x.unsqueeze(0) \n",
        "        drop_par = self.embedding(x)\n",
        "        embedding = self.dropout(drop_par) # embedding shape: (1, N, embedding_size)\n",
        "        outputs, (hidden, cell) = self.rnn(self.dropout(drop_par), (hidden, cell)) # outputs shape: (1, N, hidden_size)\n",
        "        #predictions = self.fc(outputs)\n",
        "        \n",
        "        \n",
        "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
        "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
        "        # just gonna remove the first dim\n",
        "        predictions = self.fc(outputs).squeeze(0)\n",
        "\n",
        "        return predictions, hidden, cell\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.251658Z",
          "iopub.execute_input": "2023-05-17T12:52:33.252215Z",
          "iopub.status.idle": "2023-05-17T12:52:33.278830Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.252182Z",
          "shell.execute_reply": "2023-05-17T12:52:33.277729Z"
        },
        "trusted": true,
        "id": "khzeX73vkWDf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRU**"
      ],
      "metadata": {
        "id": "WgorLRkbtgSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderGRU(nn.Module): \n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "        super(EncoderGRU, self).__init__()\n",
        "        drop_par = p\n",
        "        self.dropout = nn.Dropout(drop_par)\n",
        "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, dropout= drop_par)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (seq_length, N) where N is batch size\n",
        "        drop_par = self.embedding(x)\n",
        "        embedding = self.dropout(drop_par) # embedding shape: (seq_length, N, embedding_size)\n",
        "        outputs, hidden = self.rnn(self.dropout(drop_par)) # outputs shape: (seq_length, N, hidden_size)\n",
        "        return hidden\n",
        "\n",
        "class Seq2SeqGR(nn.Module):\n",
        "    \n",
        "    def __init__(self, encoder, decoder):\n",
        "        \n",
        "        super(Seq2SeqGR, self).__init__()\n",
        "        self.decoder, self.encoder  = decoder, encoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.2):\n",
        "        batch_size, target_len, target_vocab_size = source.shape[1], target.shape[0], num_decoder_tokens\n",
        "        x = target[0]  # Grab the first input to the Decoder which will be <SOS> token\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "        hidden= self.encoder(source)\n",
        "        \n",
        "        for t in range(1, target_len):\n",
        "            # Use previous hidden, cell as context from encoder at start\n",
        "            output, hidden = self.decoder(x, hidden)\n",
        "\n",
        "            # Store next output prediction\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            outputs[t], best_guess = output, output.argmax(1)\n",
        "            \n",
        "            x = best_guess if random.random() >= teacher_force_ratio else target[t]\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class DecoderGRU(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
        "        super(DecoderGRU, self).__init__()\n",
        "        drop_par , hidden_layer_size = p, hidden_size\n",
        "        self.dropout, self.num_layers, self.hidden_size = nn.Dropout(drop_par), num_layers, hidden_layer_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, dropout= drop_par)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = x.unsqueeze(0) # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
        "        # is 1 here because we are sending in a single word and not a sentence\n",
        "        drop_par = self.embedding(x)\n",
        "        embedding = self.dropout(drop_par) # embedding shape: (1, N, embedding_size)\n",
        "        outputs, hidden = self.rnn(self.dropout(drop_par), hidden) # outputs shape: (1, N, hidden_size)\n",
        "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
        "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
        "        # just gonna remove the first dim\n",
        "        # predictions = self.softmax(predictions)\n",
        "        predictions = self.fc(outputs).squeeze(0)\n",
        "        return predictions, hidden\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.280326Z",
          "iopub.execute_input": "2023-05-17T12:52:33.280932Z",
          "iopub.status.idle": "2023-05-17T12:52:33.302194Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.280899Z",
          "shell.execute_reply": "2023-05-17T12:52:33.301053Z"
        },
        "trusted": true,
        "id": "ro3EQCVFkWDg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RNN**"
      ],
      "metadata": {
        "id": "zv-bkJ1outmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        drop_par, hidden_layer_size = p , hidden_size\n",
        "        self.dropout, self.num_layers, self.hidden_size = nn.Dropout(p), num_layers, hidden_layer_size \n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout= drop_par)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = x.unsqueeze(0)# x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
        "        # is 1 here because we are sending in a single word and not a sentence\n",
        "        drop_par = self.embedding(x)\n",
        "        embedding = self.dropout(drop_par)\n",
        "        # embedding shape: (1, N, embedding_size)\n",
        "\n",
        "        outputs, hidden = self.rnn(self.dropout(drop_par), hidden)\n",
        "        #predictions = self.fc(outputs)\n",
        "        predictions = self.fc(outputs).squeeze(0)\n",
        "\n",
        "        return predictions, hidden\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        drop_par ,hidden_layer_size = p,hidden_size\n",
        "        self.dropout, self.num_layers, self.hidden_size = nn.Dropout(drop_par), num_layers, hidden_layer_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout= drop_par)\n",
        "\n",
        "    def forward(self, x):# x shape: (seq_length, N) where N is batch size\n",
        "        drop_par = self.embedding(x)\n",
        "        embedding = self.dropout(drop_par) # embedding shape: (seq_length, N, embedding_size)\n",
        "        outputs, hidden = self.rnn(self.dropout(drop_par))\n",
        "        # outputs shape: (seq_length, N, hidden_size)\n",
        "        # hidden shape: (num_layers, N, hidden_size)\n",
        "        return hidden"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.303738Z",
          "iopub.execute_input": "2023-05-17T12:52:33.304351Z",
          "iopub.status.idle": "2023-05-17T12:52:33.319905Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.304303Z",
          "shell.execute_reply": "2023-05-17T12:52:33.318753Z"
        },
        "trusted": true,
        "id": "MqX_xhRikWDg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model, word, input_char_index, output_char_index, reverse_input_char_index, \n",
        "              reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n",
        "              num_encoder_tokens, num_decoder_tokens, device):\n",
        "    \n",
        "    data, word_t = np.zeros((max_encoder_seq_length,1), dtype= d_type), ''\n",
        "    t_z = 0\n",
        "    for t, char in enumerate(word):\n",
        "        data[t, 0] = input_char_index[char]\n",
        "    t_z = t+1   \n",
        "    data[t_z :,0] = input_char_index[c_end]\n",
        "    \n",
        "    data = torch.tensor(data,dtype = td_type).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(data)\n",
        "    out_t = output_char_index['\\t']    \n",
        "    out_chr_reshape = np.array(out_t).reshape(1,)    \n",
        "    x = torch.tensor(out_chr_reshape).to(device)\n",
        "\n",
        "    for t in range(1, max_decoder_seq_length):\n",
        "        \n",
        "        output, hidden, cell = model.decoder(x, hidden, cell)\n",
        "        # best_guess = output.argmax(1)\n",
        "        ch = reverse_target_char_index[output.argmax(1).item()]\n",
        "        \n",
        "        if ch != '\\n':\n",
        "            word_t = word_t+ch\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return word_t"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.321368Z",
          "iopub.execute_input": "2023-05-17T12:52:33.322030Z",
          "iopub.status.idle": "2023-05-17T12:52:33.334447Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.321998Z",
          "shell.execute_reply": "2023-05-17T12:52:33.333478Z"
        },
        "trusted": true,
        "id": "-xiwgmT-kWDh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translateGR(model, word, input_char_index, output_char_index, reverse_input_char_index, \n",
        "              reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n",
        "              num_encoder_tokens, num_decoder_tokens, device):\n",
        "    \n",
        "    data, word_t = np.zeros((max_encoder_seq_length,1), dtype= d_type), ''\n",
        "    t_z = 0\n",
        "    for t, char in enumerate(word):\n",
        "        data[t, 0] = input_char_index[char]\n",
        "    t_z = t+1   \n",
        "    data[t_z :,0] = input_char_index[c_end]\n",
        "    \n",
        "    data = torch.tensor(data,dtype = td_type).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = model.encoder(data)\n",
        "    out_t = output_char_index['\\t']\n",
        "    out_reshape = np.array(out_t).reshape(1,)\n",
        "    x = torch.tensor(out_reshape).to(device)\n",
        "    \n",
        "    for t in range(1, max_decoder_seq_length):\n",
        "        output, hidden = model.decoder(x, hidden)\n",
        "        #best_guess = output.argmax(1)\n",
        "        \n",
        "        ch = reverse_target_char_index[output.argmax(1).item()]\n",
        "        if ch != '\\n':\n",
        "            word_t = word_t+ch\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return word_t"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.335868Z",
          "iopub.execute_input": "2023-05-17T12:52:33.336441Z",
          "iopub.status.idle": "2023-05-17T12:52:33.347680Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.336373Z",
          "shell.execute_reply": "2023-05-17T12:52:33.346610Z"
        },
        "trusted": true,
        "id": "kk9KsW9CkWDh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(model, word, input_char_index, output_char_index, reverse_input_char_index,reverse_target_char_index, max_encoder_seq_length, \n",
        "                max_decoder_seq_length,num_encoder_tokens, num_decoder_tokens, beam_width, device, length_penalty=0.6):\n",
        "\n",
        "\n",
        "    # Encode the input word\n",
        "    data, word_t = np.zeros((max_encoder_seq_length, 1), dtype= d_type), ''\n",
        "    t_z , bw = 0, beam_width\n",
        "    for t, char in enumerate(word):\n",
        "        data[t, 0] = input_char_index[char]\n",
        "    t_z = t+1\n",
        "    data[t_z:, 0] = input_char_index[c_end]\n",
        "\n",
        "    data = torch.tensor(data, dtype= td_type ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden,cell = model.encoder(data)\n",
        "\n",
        "    # Initialize beam\n",
        "    out_t = output_char_index['\\t']\n",
        "    out_reshape = np.array(out_t).reshape(1,)\n",
        "    hidden_par = hidden.unsqueeze(0)\n",
        "    initial_sequence = torch.tensor(out_reshape).to(device)\n",
        "    beam = [(0.0, initial_sequence, hidden_par)]  # [(score, sequence, hidden)]\n",
        "\n",
        "    for _ in range(max_decoder_seq_length):\n",
        "      candidates = []\n",
        "      for score, seq, hidden in beam:\n",
        "          # last_token = seq[-1].item()\n",
        "          if seq[-1].item() == output_char_index['\\n']:\n",
        "              # If the sequence ends with the end token, add it to the candidates\n",
        "              candidates.append((score, seq, hidden))\n",
        "              continue\n",
        "          lt_reshape = np.array(seq[-1].item()).reshape(1,) # last_token = seq[-1].item()\n",
        "          hidden_upar = hidden.squeeze(0)\n",
        "          x = torch.tensor(lt_reshape).to(device)\n",
        "          output, hidden,cell = model.decoder(x, hidden_upar,cell)\n",
        "          probabilities = F.softmax(output, dim=1)\n",
        "          cal_cand = 0\n",
        "          # Get the top-k probabilities and tokens\n",
        "          topk_probs, topk_tokens = torch.topk(probabilities, k=bw)\n",
        "\n",
        "          for prob, token in zip(topk_probs[0], topk_tokens[0]):\n",
        "              cal_cand = prob\n",
        "              n_hidden = hidden.clone()\n",
        "              new_seq = torch.cat((seq, token.unsqueeze(0)), dim=0)\n",
        "              ln_ns = len(new_seq)\n",
        "              ln_pf = ((ln_ns - 1) / 5)\n",
        "              new_hidden = n_hidden.unsqueeze(0)\n",
        "              #length_penalty_factor = ln_pf ** length_penalty  # Adjust penalty factor as needed\n",
        "              candidate_cal = score + torch.log(prob).item() / (ln_pf ** length_penalty)\n",
        "              candidates.append((candidate_cal, new_seq, new_hidden))\n",
        "\n",
        "      beam = heapq.nlargest(bw, candidates, key=lambda x: x[0])# Select top-k candidates based on the accumulated scores\n",
        "\n",
        "      \n",
        "    best_score, best_sequence, _ = max(beam, key=lambda x: x[0]) # Select the best sequence from the beam as the output\n",
        "    \n",
        "    cal_score = best_score\n",
        "    word_t = ''.join([reverse_target_char_index[token.item()] for token in best_sequence[1:-1]])\n",
        "    cal_score = 0\n",
        "    return word_t"
      ],
      "metadata": {
        "id": "Fva4i8jivqN3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_searchGR(model, word, input_char_index, output_char_index, reverse_input_char_index,\n",
        "                reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length,\n",
        "                num_encoder_tokens, num_decoder_tokens, beam_width, device, length_penalty=0.6):\n",
        "\n",
        "    # Encode the input word\n",
        "    data, word_t = np.zeros((max_encoder_seq_length, 1), dtype= d_type), ''\n",
        "    t_z, tab_pad, nl_pad,prob_dim, bw = 0, '\\t', '\\n', 1, beam_width\n",
        "    for t, char in enumerate(word):\n",
        "        data[t, 0] = input_char_index[char]\n",
        "    t_z = t+1\n",
        "    data[t_z :, 0] = input_char_index[c_end]\n",
        "\n",
        "    data = torch.tensor(data, dtype= td_type).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = model.encoder(data)\n",
        "\n",
        "    # Initialize beam\n",
        "    out_t = output_char_index[tab_pad]\n",
        "    out_reshape = np.array(out_t).reshape(1,)\n",
        "    hidden_par = hidden.unsqueeze(0)\n",
        "    initial_sequence = torch.tensor(out_reshape).to(device)\n",
        "    beam = [(0.0, initial_sequence, hidden_par)]  # [(score, sequence, hidden)]\n",
        "\n",
        "    for _ in range(max_decoder_seq_length):\n",
        "      score_cal , candidates = 0, []\n",
        "      for score, seq, hidden in beam:\n",
        "          score_cal = score_cal + score\n",
        "          last_token = seq[-1].item()\n",
        "          if last_token == output_char_index[nl_pad]:\n",
        "              # If the sequence ends with the end token, add it to the candidates\n",
        "              candidates.append((score, seq, hidden))\n",
        "              \n",
        "              continue\n",
        "          \n",
        "          last_t_reshape = np.array(last_token).reshape(1,)\n",
        "          hidden_par = hidden.squeeze(0)\n",
        "          x = torch.tensor(last_t_reshape).to(device)\n",
        "          output, hidden = model.decoder(x, hidden_par)\n",
        "          probabilities = F.softmax(output, dim = prob_dim)\n",
        "\n",
        "          # Get the top-k probabilities and tokens\n",
        "          topk_probs, topk_tokens = torch.topk(probabilities, k= bw)\n",
        "          cal_score = score\n",
        "          for prob, token in zip(topk_probs[0], topk_tokens[0]):\n",
        "              cal_score = cal_score + prob\n",
        "              new_seq = torch.cat((seq, token.unsqueeze(0)), dim=0)\n",
        "              n_hidden = hidden.clone()\n",
        "              new_seq_ln  = len(new_seq)\n",
        "              pen_fac = (new_seq_ln - 1) / 5\n",
        "              new_hidden = n_hidden.unsqueeze(0)\n",
        "              #length_penalty_factor = ((pen_fac) ** length_penalty)  # Adjust penalty factor as needed\n",
        "              \n",
        "              candidates.append((score + torch.log(prob).item() / ((pen_fac) ** length_penalty), new_seq, new_hidden))\n",
        "              cal_score = 0\n",
        "      # Select top-k candidates based on the accumulated scores\n",
        "      beam = heapq.nlargest(beam_width, candidates, key = lambda x: x[0])\n",
        "\n",
        "    \n",
        "    best_score, best_sequence, _ = max(beam, key=lambda x: x[0])# Select the best sequence from the beam as the output\n",
        "    \n",
        "    word_t = ''.join([reverse_target_char_index[token.item()] for token in best_sequence[1:-1]])\n",
        "\n",
        "    return word_t"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.349149Z",
          "iopub.execute_input": "2023-05-17T12:52:33.349864Z",
          "iopub.status.idle": "2023-05-17T12:52:33.370498Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.349831Z",
          "shell.execute_reply": "2023-05-17T12:52:33.369373Z"
        },
        "trusted": true,
        "id": "s_Bv6TU-kWDi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, batch_size, learning_rate = 2, 32, 0.001\n",
        "load_model, training= False, False\n",
        "input_size_encoder, input_size_decoder = num_encoder_tokens, num_decoder_tokens\n",
        "hidden_size = 256  # Needs to be the same for both RNN's\n",
        "output_size = num_decoder_tokens\n",
        "dec_dropout, enc_dropout = 0.1, 0.1\n",
        "encoder_embedding_size, decoder_embedding_size = 256, 256\n",
        "num_dec_layers,num_enc_layers = 2, 2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.394192Z",
          "iopub.execute_input": "2023-05-17T12:52:33.394891Z",
          "iopub.status.idle": "2023-05-17T12:52:33.406200Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.394858Z",
          "shell.execute_reply": "2023-05-17T12:52:33.405145Z"
        },
        "trusted": true,
        "id": "f2iN4tbdkWDj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(num_encoder_tokens,input_embedding_size, dp, cell_type, hidden_size, num_enc_layers, num_dec_layers,num_epochs,output_size,input_size_decoder,batch_size,beam_width):\n",
        "    #Decoder Selection\n",
        "    if(cell_type==\"LSTM\"):\n",
        "        decoder_net = Decoder(input_size_decoder,input_embedding_size,hidden_size,output_size,num_dec_layers,dp).to(device)\n",
        "    elif(cell_type==\"GRU\"):\n",
        "        decoder_net = DecoderGRU(input_size_decoder,decoder_embedding_size,hidden_size,output_size,num_dec_layers,dec_dropout).to(device)\n",
        "    else:\n",
        "        decoder_net = DecoderRNN(input_size_decoder,decoder_embedding_size,hidden_size,output_size,num_dec_layers,dec_dropout).to(device)\n",
        "    #Encoder Selection\n",
        "    if(cell_type==\"LSTM\"):\n",
        "        encoder_net = Encoder(input_size_encoder,input_embedding_size, hidden_size, num_enc_layers,dp).to(device)\n",
        "    elif(cell_type==\"GRU\"):\n",
        "        encoder_net = EncoderGRU(input_size_encoder, encoder_embedding_size, hidden_size, num_enc_layers, enc_dropout).to(device)\n",
        "    else:\n",
        "        encoder_net = EncoderRNN(input_size_encoder, encoder_embedding_size, hidden_size, num_enc_layers, enc_dropout).to(device)\n",
        "    \n",
        "    #Model Selection\n",
        "    if(cell_type==\"LSTM\"):    \n",
        "        model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
        "    else:\n",
        "        model = Seq2SeqGR(encoder_net, decoder_net).to(device)\n",
        "    \n",
        "    split_dim, bs,  m_norm = 1, batch_size, 1\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    train_ds_y, train_ds_x = torch.split(target_data, bs, dim = split_dim), torch.split(input_data, bs, dim = split_dim)\n",
        "    correct_prediction  = 0\n",
        "    #print(train_ds_x)\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "        model.eval()\n",
        "        total_count = len(val_X)\n",
        "        model.train()\n",
        "        for i, (x,y) in enumerate(zip(train_ds_x,train_ds_y)):\n",
        "            # Get input and targets and get to cuda\n",
        "            target, inp_data = y.to(device), x.to(device)\n",
        "            # Forward prop\n",
        "            output = model(inp_data, target)\n",
        "            target, output = target[1:].reshape(-1),  output[1:].reshape(-1, output.shape[2])\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward() # Back prop\n",
        "            # Clip to avoid exploding gradient issues, makes sure grads are\n",
        "            # within a healthy range\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = m_norm)\n",
        "            # Gradient descent step\n",
        "            optimizer.step()\n",
        "        correct_pred, total_words, lstm_bw = 0, total_count, 1\n",
        "        model.eval()\n",
        "        for i in range(total_count):\n",
        "            if(cell_type==\"LSTM\"):\n",
        "                decoded_sentence = beam_search(model,val_X[i], input_char_index, output_char_index, reverse_input_char_index, reverse_target_char_index, \n",
        "                                                     max_encoder_seq_length, max_decoder_seq_length,num_encoder_tokens, num_decoder_tokens,lstm_bw,device)\n",
        "            else:\n",
        "                 decoded_sentence = beam_searchGR(model,val_X[i], input_char_index, output_char_index, reverse_input_char_index, reverse_target_char_index, \n",
        "                                                        max_encoder_seq_length, max_decoder_seq_length,num_encoder_tokens, num_decoder_tokens,beam_width,device)\n",
        "                 \n",
        "            if decoded_sentence == val_Y[i][1:-1]:\n",
        "                correct_pred = correct_pred + 1\n",
        "        test_accuracy = correct_pred / total_words\n",
        "        print(correct_pred / total_words)  # test_accuracy = correct_pred / total_words\n",
        "        wandb.log({'Val_accuracy' : test_accuracy*100})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.407571Z",
          "iopub.execute_input": "2023-05-17T12:52:33.408200Z",
          "iopub.status.idle": "2023-05-17T12:52:33.429881Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.408167Z",
          "shell.execute_reply": "2023-05-17T12:52:33.428780Z"
        },
        "trusted": true,
        "id": "sWLWrPwukWDj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'parameters': {\n",
        "                   'dec_num_layers':{\n",
        "                      'values': [1,2,3] \n",
        "                      },\n",
        "                   'embedding_size': {\n",
        "                      'values': [128, 256, 512] \n",
        "                      },\n",
        "                   'hidden_size': {\n",
        "                      'values': [128, 256, 512]\n",
        "                      },\n",
        "                   'dropout': {\n",
        "                      'values': [0.1, 0.2, 0.3, 0.4]\n",
        "                      },\n",
        "                   'cell_type': {\n",
        "                      'values': ['RNN','LSTM','GRU']\n",
        "                      },\n",
        "                   'beam_search':{\n",
        "                      'values':[1,2,3,4,5]\n",
        "                      },\n",
        "                   'epochs' :{\n",
        "                      'values':[10,20,30,40]\n",
        "                      },\n",
        "                   'enc_num_layers': {\n",
        "                      'values': [1,2,3]\n",
        "                      },\n",
        "                   'batch_size': {\n",
        "                      'values': [128,256,512]\n",
        "                      }\n",
        "                },\n",
        "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'}\n",
        "}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.431366Z",
          "iopub.execute_input": "2023-05-17T12:52:33.431981Z",
          "iopub.status.idle": "2023-05-17T12:52:33.443000Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.431950Z",
          "shell.execute_reply": "2023-05-17T12:52:33.441986Z"
        },
        "trusted": true,
        "id": "SD6WwN4akWDj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    var1 = wandb.init(project=\"Assignment3_ltry\")\n",
        "    var2 = var1.config\n",
        "    # var2 is a variable that holds and saves hyperparameters and inputs\n",
        "    wandb.run.name = 'Cell_Type:-' + var2.cell_type + ', Batch_Size:-' + str(var2.batch_size) + ', Epochs:-' + str(var2.epochs) + ', Dropout:-' + str(var2.dropout) + ', Beam_Search:-' + str(var2.beam_search) +', Embedding_Size:-' + str(var2.embedding_size) + ', Hidden_Size:-' + str(var2.hidden_size) + ', Enc_Num_Layers:-' + str(var2.enc_num_layers) + ', Dec_Num_Layers:-' + str(var2.dec_num_layers)\n",
        "    if(var2.cell_type == 'RNN'):\n",
        "        epochs = 10\n",
        "        training(input_size_encoder ,var2.embedding_size, var2.dropout, var2.cell_type, var2.hidden_size, var2.enc_num_layers,  var2.enc_num_layers,epochs,num_decoder_tokens,num_decoder_tokens,var2.batch_size,var2.beam_search)\n",
        "    else:\n",
        "        training(input_size_encoder ,var2.embedding_size, var2.dropout, var2.cell_type, var2.hidden_size, var2.enc_num_layers,  var2.enc_num_layers,var2.epochs,num_decoder_tokens,num_decoder_tokens,var2.batch_size,var2.beam_search)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.444385Z",
          "iopub.execute_input": "2023-05-17T12:52:33.444918Z",
          "iopub.status.idle": "2023-05-17T12:52:33.457678Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.444887Z",
          "shell.execute_reply": "2023-05-17T12:52:33.456637Z"
        },
        "trusted": true,
        "id": "GLFi6HUmkWDj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"Assignment3_ltry\")\n",
        "wandb.agent(sweep_id, train, count=10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:53:07.727651Z",
          "iopub.execute_input": "2023-05-17T12:53:07.729967Z",
          "iopub.status.idle": "2023-05-17T13:22:33.534849Z",
          "shell.execute_reply.started": "2023-05-17T12:53:07.729930Z",
          "shell.execute_reply": "2023-05-17T13:22:33.533777Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3186a016a4484ef48d98b6ce4ac0d629",
            "1a181a0ded8c4dd18c11acc55978662a",
            "ae096a69596c4a05bed6da2d4be9975b",
            "0a6b49e5b90d4f63b0b4043619f8ce95",
            "46775b5759d3456b9c342ed72e01e29c",
            "556d3cab10464f70a428a04a60aeb889",
            "2a13a586326a46e2b19f958bb9d8763a",
            "81814895e5104850a50cbf8c5d6364c3",
            "6d7c0b4063e84ce3a27ac4d719b7dd0f",
            "3af13a92456841e8954cb91e2e4e2b03",
            "cb8ec0b65827474b814034eb559feb64",
            "8a73cceda6d74324a52b397b010e227f",
            "49f3da8f202e40c884744b15cf6ed699",
            "9bdc014e0fcc4fe7867fb4f5fa65f807",
            "6001d6d6cf864fdf94ac52d129b036bf",
            "f3a38cde893d47e5bc53ee68dde92fa3"
          ]
        },
        "id": "MVw2KTdOkWDk",
        "outputId": "09078029-4f7a-4ec1-d3cd-39a572e93957"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: fiun7jl0\n",
            "Sweep URL: https://wandb.ai/cs22m083/Assignment3_ltry/sweeps/fiun7jl0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xods2cu6 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_num_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_num_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n",
            "Exception in thread NetStatThr:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "Exception in thread ChkStopThr:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 278, in check_stop_status\n",
            "    self._loop_check_status(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 216, in _loop_check_status\n",
            "    local_handle = request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface.py\", line 787, in deliver_stop_status\n",
            "    return self._deliver_stop_status(status)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 585, in _deliver_stop_status\n",
            "        self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 260, in check_network_status\n",
            "return self._deliver_record(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 560, in _deliver_record\n",
            "    self._loop_check_status(    \n",
            "handle = mailbox._deliver_record(record, interface=self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 216, in _loop_check_status\n",
            "    interface._publish(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
            "    local_handle = request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface.py\", line 795, in deliver_network_status\n",
            "        self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
            "return self._deliver_network_status(status)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 601, in _deliver_network_status\n",
            "        self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
            "return self._deliver_record(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 560, in _deliver_record\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
            "        handle = mailbox._deliver_record(record, interface=self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n",
            "self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "BrokenPipeError:     interface._publish(record)[Errno 32] Broken pipe\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/aksharantar_dataset/tel/wandb/run-20230518_092517-xods2cu6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22m083/Assignment3_ltry/runs/xods2cu6' target=\"_blank\">easy-sweep-1</a></strong> to <a href='https://wandb.ai/cs22m083/Assignment3_ltry' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m083/Assignment3_ltry/sweeps/fiun7jl0' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_ltry/sweeps/fiun7jl0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22m083/Assignment3_ltry' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_ltry</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/cs22m083/Assignment3_ltry/sweeps/fiun7jl0' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_ltry/sweeps/fiun7jl0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22m083/Assignment3_ltry/runs/xods2cu6' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_ltry/runs/xods2cu6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0 / 30]\n",
            "0.0\n",
            "[Epoch 1 / 30]\n",
            "0.013427734375\n",
            "[Epoch 2 / 30]\n",
            "0.1103515625\n",
            "[Epoch 3 / 30]\n",
            "0.22216796875\n",
            "[Epoch 4 / 30]\n",
            "0.313720703125\n",
            "[Epoch 5 / 30]\n",
            "0.374755859375\n",
            "[Epoch 6 / 30]\n",
            "0.3505859375\n",
            "[Epoch 7 / 30]\n",
            "0.428955078125\n",
            "[Epoch 8 / 30]\n",
            "0.4306640625\n",
            "[Epoch 9 / 30]\n",
            "0.46484375\n",
            "[Epoch 10 / 30]\n",
            "0.45751953125\n",
            "[Epoch 11 / 30]\n",
            "0.42041015625\n",
            "[Epoch 12 / 30]\n",
            "0.48876953125\n",
            "[Epoch 13 / 30]\n",
            "0.4560546875\n",
            "[Epoch 14 / 30]\n",
            "0.492919921875\n",
            "[Epoch 15 / 30]\n",
            "0.5009765625\n",
            "[Epoch 16 / 30]\n",
            "0.51708984375\n",
            "[Epoch 17 / 30]\n",
            "0.511962890625\n",
            "[Epoch 18 / 30]\n",
            "0.513671875\n",
            "[Epoch 19 / 30]\n",
            "0.487548828125\n",
            "[Epoch 20 / 30]\n",
            "0.495849609375\n",
            "[Epoch 21 / 30]\n",
            "0.513671875\n",
            "[Epoch 22 / 30]\n",
            "0.518310546875\n",
            "[Epoch 23 / 30]\n",
            "0.515380859375\n",
            "[Epoch 24 / 30]\n",
            "0.501953125\n",
            "[Epoch 25 / 30]\n",
            "0.52490234375\n",
            "[Epoch 26 / 30]\n",
            "0.524169921875\n",
            "[Epoch 27 / 30]\n",
            "0.5078125\n",
            "[Epoch 28 / 30]\n",
            "0.4853515625\n",
            "[Epoch 29 / 30]\n",
            "0.510986328125\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3186a016a4484ef48d98b6ce4ac0d629"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Val_accuracy</td><td>▁▁▂▄▅▆▆▇▇▇▇▇█▇██████████████▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Val_accuracy</td><td>51.09863</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">easy-sweep-1</strong> at: <a href='https://wandb.ai/cs22m083/Assignment3_ltry/runs/xods2cu6' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_ltry/runs/xods2cu6</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230518_092517-xods2cu6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: un2qbp5t with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_num_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_num_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/aksharantar_dataset/tel/wandb/run-20230518_103552-un2qbp5t</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22m083/Assignment3_ltry/runs/un2qbp5t' target=\"_blank\">magic-sweep-2</a></strong> to <a href='https://wandb.ai/cs22m083/Assignment3_ltry' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m083/Assignment3_ltry/sweeps/fiun7jl0' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_ltry/sweeps/fiun7jl0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22m083/Assignment3_ltry' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_ltry</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/cs22m083/Assignment3_ltry/sweeps/fiun7jl0' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_ltry/sweeps/fiun7jl0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22m083/Assignment3_ltry/runs/un2qbp5t' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_ltry/runs/un2qbp5t</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0 / 10]\n",
            "0.0\n",
            "[Epoch 1 / 10]\n",
            "0.0\n",
            "[Epoch 2 / 10]\n",
            "0.0\n",
            "[Epoch 3 / 10]\n",
            "0.003662109375\n",
            "[Epoch 4 / 10]\n",
            "0.0625\n",
            "[Epoch 5 / 10]\n",
            "0.20556640625\n",
            "[Epoch 6 / 10]\n",
            "0.298583984375\n",
            "[Epoch 7 / 10]\n",
            "0.370849609375\n",
            "[Epoch 8 / 10]\n",
            "0.41259765625\n",
            "[Epoch 9 / 10]\n",
            "0.411376953125\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d7c0b4063e84ce3a27ac4d719b7dd0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Val_accuracy</td><td>▁▁▁▁▂▄▆▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Val_accuracy</td><td>41.1377</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">magic-sweep-2</strong> at: <a href='https://wandb.ai/cs22m083/Assignment3_ltry/runs/un2qbp5t' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_ltry/runs/un2qbp5t</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230518_103552-un2qbp5t/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0w7w94sv with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_num_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_num_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/aksharantar_dataset/tel/wandb/run-20230518_104746-0w7w94sv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22m083/Assignment3_ltry/runs/0w7w94sv' target=\"_blank\">zany-sweep-3</a></strong> to <a href='https://wandb.ai/cs22m083/Assignment3_ltry' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m083/Assignment3_ltry/sweeps/fiun7jl0' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_ltry/sweeps/fiun7jl0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22m083/Assignment3_ltry' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_ltry</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/cs22m083/Assignment3_ltry/sweeps/fiun7jl0' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_ltry/sweeps/fiun7jl0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22m083/Assignment3_ltry/runs/0w7w94sv' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_ltry/runs/0w7w94sv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0 / 40]\n",
            "0.0\n",
            "[Epoch 1 / 40]\n",
            "0.0009765625\n",
            "[Epoch 2 / 40]\n",
            "0.010986328125\n",
            "[Epoch 3 / 40]\n",
            "0.059326171875\n",
            "[Epoch 4 / 40]\n",
            "0.158447265625\n",
            "[Epoch 5 / 40]\n",
            "0.220947265625\n",
            "[Epoch 6 / 40]\n",
            "0.27880859375\n",
            "[Epoch 7 / 40]\n",
            "0.326171875\n",
            "[Epoch 8 / 40]\n",
            "0.361328125\n",
            "[Epoch 9 / 40]\n",
            "0.386962890625\n",
            "[Epoch 10 / 40]\n",
            "0.383056640625\n",
            "[Epoch 11 / 40]\n",
            "0.395263671875\n",
            "[Epoch 12 / 40]\n",
            "0.4443359375\n",
            "[Epoch 13 / 40]\n",
            "0.44775390625\n",
            "[Epoch 14 / 40]\n",
            "0.46337890625\n",
            "[Epoch 15 / 40]\n",
            "0.457763671875\n",
            "[Epoch 16 / 40]\n",
            "0.451904296875\n",
            "[Epoch 17 / 40]\n",
            "0.4033203125\n",
            "[Epoch 18 / 40]\n",
            "0.477294921875\n",
            "[Epoch 19 / 40]\n",
            "0.494384765625\n",
            "[Epoch 20 / 40]\n",
            "0.499267578125\n",
            "[Epoch 21 / 40]\n",
            "0.509521484375\n",
            "[Epoch 22 / 40]\n",
            "0.507568359375\n",
            "[Epoch 23 / 40]\n",
            "0.516845703125\n",
            "[Epoch 24 / 40]\n",
            "0.4697265625\n",
            "[Epoch 25 / 40]\n",
            "0.51025390625\n",
            "[Epoch 26 / 40]\n",
            "0.511962890625\n",
            "[Epoch 27 / 40]\n",
            "0.518798828125\n",
            "[Epoch 28 / 40]\n",
            "0.512451171875\n",
            "[Epoch 29 / 40]\n",
            "0.525146484375\n",
            "[Epoch 30 / 40]\n",
            "0.51611328125\n",
            "[Epoch 31 / 40]\n",
            "0.51806640625\n",
            "[Epoch 32 / 40]\n",
            "0.5146484375\n",
            "[Epoch 33 / 40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in callback <function _WandbInit._pause_backend at 0x7efd524a57e0> (for post_run_cell):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BrokenPipeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/backcall/backcall.py\u001b[0m in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36m_pause_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pausing backend\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_resume_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36mpublish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpublish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0mpause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPauseRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_publish_pause\u001b[0;34m(self, pause)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpause\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPauseRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResumeRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_sock.py\u001b[0m in \u001b[0;36m_publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pb.Record\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_record_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     def _communicate_async(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36msend_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mserver_req\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mServerRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mserver_req\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_publish\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_server_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_req\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_extract_packet_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36msend_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_server_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_server_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36m_send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<BI\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sendall_with_error_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_server_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36m_sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0;31m# sent equal to 0 indicates a closed socket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "E0Wd-KjzkWDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TARAEYZqkWDk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}