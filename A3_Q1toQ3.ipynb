{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import csv\n",
        "!pip install wandb\n",
        "import wandb\n",
        "import heapq\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.ticker as ticker\n",
        "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:18.051562Z",
          "iopub.execute_input": "2023-05-17T12:52:18.052003Z",
          "iopub.status.idle": "2023-05-17T12:52:30.712510Z",
          "shell.execute_reply.started": "2023-05-17T12:52:18.051962Z",
          "shell.execute_reply": "2023-05-17T12:52:30.711467Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfvFRD6VkWDP",
        "outputId": "90a5a34d-fd5b-4aed-eaf9-6a9c0e3a6850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.23.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key='b032cc059132c9aac4f1b317f6f9ad007ef9e4d4')\n",
        "wandb.init(project=\"Assignment3_final\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "zFb-7o4FJTsi",
        "outputId": "e5241703-f9cd-4aad-c4ed-4b6af9181d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs22m083\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230518_092341-69lj20tj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22m083/Assignment3_ltry/runs/69lj20tj' target=\"_blank\">grateful-microwave-8</a></strong> to <a href='https://wandb.ai/cs22m083/Assignment3_ltry' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22m083/Assignment3_ltry' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_ltry</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22m083/Assignment3_ltry/runs/69lj20tj' target=\"_blank\">https://wandb.ai/cs22m083/Assignment3_ltry/runs/69lj20tj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cs22m083/Assignment3_ltry/runs/69lj20tj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7efd5249f730>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE6g7bby0rZh",
        "outputId": "8da173d1-2440-4514-f227-d4e005673da8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/aksharantar_dataset/tel/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra6h-RxQ0vVH",
        "outputId": "d36f13e0-ca3d-4012-daf1-6a1e519c03f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/aksharantar_dataset/tel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tsv_file = open(\"/content/drive/MyDrive/aksharantar_dataset/tel/tel_train.csv\")\n",
        "test_tsv_file = open(\"/content/drive/MyDrive/aksharantar_dataset/tel/tel_test.csv\")\n",
        "val_tsv_file = open(\"/content/drive/MyDrive/aksharantar_dataset/tel/tel_valid.csv\")\n",
        "read_tsv = csv.reader(tsv_file)\n",
        "test_read_tsv = csv.reader(test_tsv_file)\n",
        "val_read_tsv = csv.reader(val_tsv_file)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:30.722634Z",
          "iopub.execute_input": "2023-05-17T12:52:30.728581Z",
          "iopub.status.idle": "2023-05-17T12:52:30.748886Z",
          "shell.execute_reply.started": "2023-05-17T12:52:30.728541Z",
          "shell.execute_reply": "2023-05-17T12:52:30.747588Z"
        },
        "trusted": true,
        "id": "RCvT2zejkWDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, train_Y = [], []\n",
        "for i in read_tsv:\n",
        "    train_X.append(i[0])\n",
        "    train_Y.append(i[1])\n",
        "    \n",
        "test_X,test_Y = [], []\n",
        "for i in test_read_tsv:\n",
        "    test_X.append(i[0])\n",
        "    test_Y.append(i[1])\n",
        "    \n",
        "val_X, val_Y = [], []\n",
        "for i in val_read_tsv:\n",
        "    val_X.append(i[0])\n",
        "    val_Y.append(i[1])\n",
        "    \n",
        "\n",
        "\n",
        "test_X, test_Y = np.array(test_X), np.array(test_Y)\n",
        "t_range = test_Y.shape[0]\n",
        "val_X, val_Y = np.array(val_X),  np.array(val_Y)\n",
        "v_range = val_Y.shape[0]\n",
        "train_X, train_Y = np.array(train_X), np.array(train_Y)\n",
        "tr_range = train_Y.shape[0]\n",
        "for i in range(t_range):\n",
        "    test_Y[i] = \"\\t\" + test_Y[i] + \"\\n\"\n",
        "    \n",
        "for i in range(v_range):\n",
        "    val_Y[i] = \"\\t\" + val_Y[i] + \"\\n\"\n",
        "\n",
        "for i in range(tr_range):\n",
        "    train_Y[i] = \"\\t\" + train_Y[i] + \"\\n\"\n",
        "ch_end = \" \""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:30.750334Z",
          "iopub.execute_input": "2023-05-17T12:52:30.750654Z",
          "iopub.status.idle": "2023-05-17T12:52:31.106901Z",
          "shell.execute_reply.started": "2023-05-17T12:52:30.750622Z",
          "shell.execute_reply": "2023-05-17T12:52:31.105912Z"
        },
        "trusted": true,
        "id": "ScpIcwmtkWDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_input_corpus, output_corpus, input_corpus,  val_output_corpus = set(), set(), set(), set()\n",
        "\n",
        "for word in train_Y:\n",
        "    for char in word:\n",
        "        if char not in output_corpus:\n",
        "            output_corpus.add(char)\n",
        "\n",
        "for word in train_X:\n",
        "    for char in word:\n",
        "        if char not in input_corpus:\n",
        "            input_corpus.add(char)\n",
        "\n",
        "output_corpus.add(ch_end)\n",
        "input_corpus.add(ch_end)\n",
        "\n",
        "\n",
        "for word in val_Y:\n",
        "    for char in word:\n",
        "        if char not in val_output_corpus:\n",
        "            val_output_corpus.add(char)\n",
        "\n",
        "output_corpus,  input_corpus = sorted(list(output_corpus)), sorted(list(input_corpus))\n",
        "num_encoder_tokens =  len(input_corpus)  \n",
        "\n",
        "for word in val_X:\n",
        "    for char in word:\n",
        "        if char not in val_input_corpus:\n",
        "            val_input_corpus.add(char)\n",
        "\n",
        "num_decoder_tokens = len(output_corpus)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:31.113255Z",
          "iopub.execute_input": "2023-05-17T12:52:31.115702Z",
          "iopub.status.idle": "2023-05-17T12:52:31.502261Z",
          "shell.execute_reply.started": "2023-05-17T12:52:31.115664Z",
          "shell.execute_reply": "2023-05-17T12:52:31.501219Z"
        },
        "trusted": true,
        "id": "o6M3XpFNkWDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_decoder_seq_length,max_encoder_seq_length = max([len(txt) for txt in train_Y]), max([len(txt) for txt in train_X]) + 2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:31.503520Z",
          "iopub.execute_input": "2023-05-17T12:52:31.504076Z",
          "iopub.status.idle": "2023-05-17T12:52:31.610035Z",
          "shell.execute_reply.started": "2023-05-17T12:52:31.504028Z",
          "shell.execute_reply": "2023-05-17T12:52:31.609121Z"
        },
        "trusted": true,
        "id": "7xXKymJfkWDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "print(\"Number of samples:\", len(train_X))\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:31.611278Z",
          "iopub.execute_input": "2023-05-17T12:52:31.611856Z",
          "iopub.status.idle": "2023-05-17T12:52:31.619583Z",
          "shell.execute_reply.started": "2023-05-17T12:52:31.611823Z",
          "shell.execute_reply": "2023-05-17T12:52:31.618578Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAweNDwakWDa",
        "outputId": "8602ec62-a8a4-441e-eb98-7aeb49134eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique input tokens: 27\n",
            "Max sequence length for outputs: 21\n",
            "Number of samples: 51200\n",
            "Max sequence length for inputs: 30\n",
            "Number of unique output tokens: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, num_epochs, learning_rate = 32, 10, 0.001\n",
        "load_model, training = False, False\n",
        "input_size_encoder, input_size_decoder = num_encoder_tokens, num_decoder_tokens\n",
        "output_size = num_decoder_tokens\n",
        "encoder_embedding_size, decoder_embedding_size = 256, 256\n",
        "hidden_size = 256  # Needs to be the same for both RNN's\n",
        "num_enc_layers, num_dec_layers = 1, 1\n",
        "enc_dropout, dec_dropout = 0.1, 0.1\n",
        "d_type, td_type = \"int64\", torch.int64"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:31.620774Z",
          "iopub.execute_input": "2023-05-17T12:52:31.621710Z",
          "iopub.status.idle": "2023-05-17T12:52:31.631916Z",
          "shell.execute_reply.started": "2023-05-17T12:52:31.621677Z",
          "shell.execute_reply": "2023-05-17T12:52:31.630727Z"
        },
        "trusted": true,
        "id": "utKX4bqkkWDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ln = len(train_X)\n",
        "val_ln = len(val_X)\n",
        "input_char_index, output_char_index = dict([(char, i) for i, char in enumerate(input_corpus)]), dict([(char, i) for i, char in enumerate(output_corpus)])\n",
        "input_data, target_data = np.zeros((max_encoder_seq_length,train_ln), dtype= d_type), np.zeros((max_decoder_seq_length,train_ln), dtype=d_type)\n",
        "c_end = \" \"\n",
        "input_data_val, target_data_val = np.zeros((max_encoder_seq_length,val_ln), dtype=d_type), np.zeros((max_decoder_seq_length,val_ln), dtype=d_type)\n",
        "for i, (x, y) in enumerate(zip(val_X, val_Y)):\n",
        "    t_n = i+ val_ln\n",
        "    for t, char in enumerate(y):\n",
        "        target_data_val[t, i] = output_char_index[char]\n",
        "    t_n = t+1       \n",
        "    target_data_val[t_n :,i] = output_char_index[c_end]\n",
        "    t_n = 0\n",
        "    for t, char in enumerate(x):\n",
        "        input_data_val[t, i] = input_char_index[char]\n",
        "    t_n = t+1   \n",
        "    input_data_val[t_n :,i] = input_char_index[c_end] \n",
        "t_n =0    \n",
        "for i, (x, y) in enumerate(zip(train_X, train_Y)):\n",
        "    t_n = i+ val_ln\n",
        "    for t, char in enumerate(y):\n",
        "        target_data[t, i] = output_char_index[char]\n",
        "    t_n  = t+1        \n",
        "    target_data[t_n :,i] = output_char_index[c_end]\n",
        "    t_n =0\n",
        "    for t, char in enumerate(x):\n",
        "        input_data[t, i] = input_char_index[char]\n",
        "    t_n = t+1   \n",
        "    input_data[t_n :,i] = input_char_index[c_end]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:31.633411Z",
          "iopub.execute_input": "2023-05-17T12:52:31.633982Z",
          "iopub.status.idle": "2023-05-17T12:52:33.170614Z",
          "shell.execute_reply.started": "2023-05-17T12:52:31.633949Z",
          "shell.execute_reply": "2023-05-17T12:52:33.169597Z"
        },
        "trusted": true,
        "id": "BafE7JfVkWDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convertin numpy arrays to tensors\n",
        "input_data_val, target_data_val = torch.tensor(input_data_val,dtype = td_type), torch.tensor(target_data_val,dtype = td_type)\n",
        "input_data, target_data = torch.tensor(input_data,dtype = td_type), torch.tensor(target_data,dtype = td_type)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.171878Z",
          "iopub.execute_input": "2023-05-17T12:52:33.172542Z",
          "iopub.status.idle": "2023-05-17T12:52:33.235548Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.172505Z",
          "shell.execute_reply": "2023-05-17T12:52:33.234579Z"
        },
        "trusted": true,
        "id": "ofDaFzv0kWDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_target_char_index,reverse_input_char_index, t_z = dict((i, char) for char, i in output_char_index.items()), dict((i, char) for char, i in input_char_index.items()), 0"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.239789Z",
          "iopub.execute_input": "2023-05-17T12:52:33.240674Z",
          "iopub.status.idle": "2023-05-17T12:52:33.250214Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.240640Z",
          "shell.execute_reply": "2023-05-17T12:52:33.249108Z"
        },
        "trusted": true,
        "id": "Sf0BRbl0kWDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM** \n"
      ],
      "metadata": {
        "id": "mtcIisUPs_8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module): \n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        drop_par = dropout\n",
        "        self.dropout = nn.Dropout(drop_par)\n",
        "        self.num_layers, self.hidden_size, = num_layers, hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n",
        "\n",
        "    def forward(self, x): # x shape: (seq_length, N) where N is batch size\n",
        "        drop_par = self.embedding(x)\n",
        "        embedding = self.dropout(drop_par) # embedding shape: (seq_length, N, embedding_size)\n",
        "        outputs, (hidden, cell) = self.rnn(self.dropout(drop_par)) # outputs shape: (seq_length, N, hidden_size)\n",
        "        return hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder):\n",
        "\n",
        "        super(Seq2Seq, self).__init__()  \n",
        "        self.decoder, self.encoder = decoder, encoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.2):\n",
        "        batch_size, target_len, target_vocab_size = source.shape[1], target.shape[0], num_decoder_tokens\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "        # Grab the first input to the Decoder which will be <SOS> token\n",
        "        x = target[0]\n",
        "        hidden, cell = self.encoder(source)\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            # Use previous hidden, cell as context from encoder at start\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "\n",
        "            # Store next output prediction\n",
        "            outputs[t], best_guess = output, output.argmax(1)\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            x = best_guess if random.random() >= teacher_force_ratio else target[t]\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        drop_par, hidden_layer_size = dropout, hidden_size\n",
        "        self.dropout,self.num_layers, self.hidden_size = nn.Dropout(drop_par),  num_layers, hidden_layer_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout= drop_par)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell): # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
        "        # is 1 here because we are sending in a single word and not a sentence\n",
        "        x  = x.unsqueeze(0) \n",
        "        drop_par = self.embedding(x)\n",
        "        embedding = self.dropout(drop_par) # embedding shape: (1, N, embedding_size)\n",
        "        outputs, (hidden, cell) = self.rnn(self.dropout(drop_par), (hidden, cell)) # outputs shape: (1, N, hidden_size)\n",
        "        #predictions = self.fc(outputs)\n",
        "        \n",
        "        \n",
        "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
        "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
        "        # just gonna remove the first dim\n",
        "        predictions = self.fc(outputs).squeeze(0)\n",
        "\n",
        "        return predictions, hidden, cell\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.251658Z",
          "iopub.execute_input": "2023-05-17T12:52:33.252215Z",
          "iopub.status.idle": "2023-05-17T12:52:33.278830Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.252182Z",
          "shell.execute_reply": "2023-05-17T12:52:33.277729Z"
        },
        "trusted": true,
        "id": "khzeX73vkWDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRU**"
      ],
      "metadata": {
        "id": "WgorLRkbtgSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderGRU(nn.Module): \n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "        super(EncoderGRU, self).__init__()\n",
        "        drop_par = p\n",
        "        self.dropout = nn.Dropout(drop_par)\n",
        "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, dropout= drop_par)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (seq_length, N) where N is batch size\n",
        "        drop_par = self.embedding(x)\n",
        "        embedding = self.dropout(drop_par) # embedding shape: (seq_length, N, embedding_size)\n",
        "        outputs, hidden = self.rnn(self.dropout(drop_par)) # outputs shape: (seq_length, N, hidden_size)\n",
        "        return hidden\n",
        "\n",
        "class Seq2SeqGR(nn.Module):\n",
        "    \n",
        "    def __init__(self, encoder, decoder):\n",
        "        \n",
        "        super(Seq2SeqGR, self).__init__()\n",
        "        self.decoder, self.encoder  = decoder, encoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.2):\n",
        "        batch_size, target_len, target_vocab_size = source.shape[1], target.shape[0], num_decoder_tokens\n",
        "        x = target[0]  # Grab the first input to the Decoder which will be <SOS> token\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "        hidden= self.encoder(source)\n",
        "        \n",
        "        for t in range(1, target_len):\n",
        "            # Use previous hidden, cell as context from encoder at start\n",
        "            output, hidden = self.decoder(x, hidden)\n",
        "\n",
        "            # Store next output prediction\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            outputs[t], best_guess = output, output.argmax(1)\n",
        "            \n",
        "            x = best_guess if random.random() >= teacher_force_ratio else target[t]\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class DecoderGRU(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
        "        super(DecoderGRU, self).__init__()\n",
        "        drop_par , hidden_layer_size = p, hidden_size\n",
        "        self.dropout, self.num_layers, self.hidden_size = nn.Dropout(drop_par), num_layers, hidden_layer_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, dropout= drop_par)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = x.unsqueeze(0) # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
        "        # is 1 here because we are sending in a single word and not a sentence\n",
        "        drop_par = self.embedding(x)\n",
        "        embedding = self.dropout(drop_par) # embedding shape: (1, N, embedding_size)\n",
        "        outputs, hidden = self.rnn(self.dropout(drop_par), hidden) # outputs shape: (1, N, hidden_size)\n",
        "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
        "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
        "        # just gonna remove the first dim\n",
        "        # predictions = self.softmax(predictions)\n",
        "        predictions = self.fc(outputs).squeeze(0)\n",
        "        return predictions, hidden\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.280326Z",
          "iopub.execute_input": "2023-05-17T12:52:33.280932Z",
          "iopub.status.idle": "2023-05-17T12:52:33.302194Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.280899Z",
          "shell.execute_reply": "2023-05-17T12:52:33.301053Z"
        },
        "trusted": true,
        "id": "ro3EQCVFkWDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RNN**"
      ],
      "metadata": {
        "id": "zv-bkJ1outmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        drop_par, hidden_layer_size = p , hidden_size\n",
        "        self.dropout, self.num_layers, self.hidden_size = nn.Dropout(p), num_layers, hidden_layer_size \n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout= drop_par)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = x.unsqueeze(0)# x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
        "        # is 1 here because we are sending in a single word and not a sentence\n",
        "        drop_par = self.embedding(x)\n",
        "        embedding = self.dropout(drop_par)\n",
        "        # embedding shape: (1, N, embedding_size)\n",
        "\n",
        "        outputs, hidden = self.rnn(self.dropout(drop_par), hidden)\n",
        "        #predictions = self.fc(outputs)\n",
        "        predictions = self.fc(outputs).squeeze(0)\n",
        "\n",
        "        return predictions, hidden\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        drop_par ,hidden_layer_size = p,hidden_size\n",
        "        self.dropout, self.num_layers, self.hidden_size = nn.Dropout(drop_par), num_layers, hidden_layer_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout= drop_par)\n",
        "\n",
        "    def forward(self, x):# x shape: (seq_length, N) where N is batch size\n",
        "        drop_par = self.embedding(x)\n",
        "        embedding = self.dropout(drop_par) # embedding shape: (seq_length, N, embedding_size)\n",
        "        outputs, hidden = self.rnn(self.dropout(drop_par))\n",
        "        # outputs shape: (seq_length, N, hidden_size)\n",
        "        # hidden shape: (num_layers, N, hidden_size)\n",
        "        return hidden"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.303738Z",
          "iopub.execute_input": "2023-05-17T12:52:33.304351Z",
          "iopub.status.idle": "2023-05-17T12:52:33.319905Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.304303Z",
          "shell.execute_reply": "2023-05-17T12:52:33.318753Z"
        },
        "trusted": true,
        "id": "MqX_xhRikWDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model, word, input_char_index, output_char_index, reverse_input_char_index, \n",
        "              reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n",
        "              num_encoder_tokens, num_decoder_tokens, device):\n",
        "    \n",
        "    data, word_t = np.zeros((max_encoder_seq_length,1), dtype= d_type), ''\n",
        "    t_z = 0\n",
        "    for t, char in enumerate(word):\n",
        "        data[t, 0] = input_char_index[char]\n",
        "    t_z = t+1   \n",
        "    data[t_z :,0] = input_char_index[c_end]\n",
        "    \n",
        "    data = torch.tensor(data,dtype = td_type).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(data)\n",
        "    out_t = output_char_index['\\t']    \n",
        "    out_chr_reshape = np.array(out_t).reshape(1,)    \n",
        "    x = torch.tensor(out_chr_reshape).to(device)\n",
        "\n",
        "    for t in range(1, max_decoder_seq_length):\n",
        "        \n",
        "        output, hidden, cell = model.decoder(x, hidden, cell)\n",
        "        # best_guess = output.argmax(1)\n",
        "        ch = reverse_target_char_index[output.argmax(1).item()]\n",
        "        \n",
        "        if ch != '\\n':\n",
        "            word_t = word_t+ch\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return word_t"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.321368Z",
          "iopub.execute_input": "2023-05-17T12:52:33.322030Z",
          "iopub.status.idle": "2023-05-17T12:52:33.334447Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.321998Z",
          "shell.execute_reply": "2023-05-17T12:52:33.333478Z"
        },
        "trusted": true,
        "id": "-xiwgmT-kWDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translateGR(model, word, input_char_index, output_char_index, reverse_input_char_index, \n",
        "              reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n",
        "              num_encoder_tokens, num_decoder_tokens, device):\n",
        "    \n",
        "    data, word_t = np.zeros((max_encoder_seq_length,1), dtype= d_type), ''\n",
        "    t_z = 0\n",
        "    for t, char in enumerate(word):\n",
        "        data[t, 0] = input_char_index[char]\n",
        "    t_z = t+1   \n",
        "    data[t_z :,0] = input_char_index[c_end]\n",
        "    \n",
        "    data = torch.tensor(data,dtype = td_type).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = model.encoder(data)\n",
        "    out_t = output_char_index['\\t']\n",
        "    out_reshape = np.array(out_t).reshape(1,)\n",
        "    x = torch.tensor(out_reshape).to(device)\n",
        "    \n",
        "    for t in range(1, max_decoder_seq_length):\n",
        "        output, hidden = model.decoder(x, hidden)\n",
        "        #best_guess = output.argmax(1)\n",
        "        \n",
        "        ch = reverse_target_char_index[output.argmax(1).item()]\n",
        "        if ch != '\\n':\n",
        "            word_t = word_t+ch\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return word_t"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.335868Z",
          "iopub.execute_input": "2023-05-17T12:52:33.336441Z",
          "iopub.status.idle": "2023-05-17T12:52:33.347680Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.336373Z",
          "shell.execute_reply": "2023-05-17T12:52:33.346610Z"
        },
        "trusted": true,
        "id": "kk9KsW9CkWDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(model, word, input_char_index, output_char_index, reverse_input_char_index,reverse_target_char_index, max_encoder_seq_length, \n",
        "                max_decoder_seq_length,num_encoder_tokens, num_decoder_tokens, beam_width, device, length_penalty=0.6):\n",
        "\n",
        "\n",
        "    # Encode the input word\n",
        "    data, word_t = np.zeros((max_encoder_seq_length, 1), dtype= d_type), ''\n",
        "    t_z , bw = 0, beam_width\n",
        "    for t, char in enumerate(word):\n",
        "        data[t, 0] = input_char_index[char]\n",
        "    t_z = t+1\n",
        "    data[t_z:, 0] = input_char_index[c_end]\n",
        "\n",
        "    data = torch.tensor(data, dtype= td_type ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden,cell = model.encoder(data)\n",
        "\n",
        "    # Initialize beam\n",
        "    out_t = output_char_index['\\t']\n",
        "    out_reshape = np.array(out_t).reshape(1,)\n",
        "    hidden_par = hidden.unsqueeze(0)\n",
        "    initial_sequence = torch.tensor(out_reshape).to(device)\n",
        "    beam = [(0.0, initial_sequence, hidden_par)]  # [(score, sequence, hidden)]\n",
        "\n",
        "    for _ in range(max_decoder_seq_length):\n",
        "      candidates = []\n",
        "      for score, seq, hidden in beam:\n",
        "          # last_token = seq[-1].item()\n",
        "          if seq[-1].item() == output_char_index['\\n']:\n",
        "              # If the sequence ends with the end token, add it to the candidates\n",
        "              candidates.append((score, seq, hidden))\n",
        "              continue\n",
        "          lt_reshape = np.array(seq[-1].item()).reshape(1,) # last_token = seq[-1].item()\n",
        "          hidden_upar = hidden.squeeze(0)\n",
        "          x = torch.tensor(lt_reshape).to(device)\n",
        "          output, hidden,cell = model.decoder(x, hidden_upar,cell)\n",
        "          probabilities = F.softmax(output, dim=1)\n",
        "          cal_cand = 0\n",
        "          # Get the top-k probabilities and tokens\n",
        "          topk_probs, topk_tokens = torch.topk(probabilities, k=bw)\n",
        "\n",
        "          for prob, token in zip(topk_probs[0], topk_tokens[0]):\n",
        "              cal_cand = prob\n",
        "              n_hidden = hidden.clone()\n",
        "              new_seq = torch.cat((seq, token.unsqueeze(0)), dim=0)\n",
        "              ln_ns = len(new_seq)\n",
        "              ln_pf = ((ln_ns - 1) / 5)\n",
        "              new_hidden = n_hidden.unsqueeze(0)\n",
        "              #length_penalty_factor = ln_pf ** length_penalty  # Adjust penalty factor as needed\n",
        "              candidate_cal = score + torch.log(prob).item() / (ln_pf ** length_penalty)\n",
        "              candidates.append((candidate_cal, new_seq, new_hidden))\n",
        "\n",
        "      beam = heapq.nlargest(bw, candidates, key=lambda x: x[0])# Select top-k candidates based on the accumulated scores\n",
        "\n",
        "      \n",
        "    best_score, best_sequence, _ = max(beam, key=lambda x: x[0]) # Select the best sequence from the beam as the output\n",
        "    \n",
        "    cal_score = best_score\n",
        "    word_t = ''.join([reverse_target_char_index[token.item()] for token in best_sequence[1:-1]])\n",
        "    cal_score = 0\n",
        "    return word_t"
      ],
      "metadata": {
        "id": "Fva4i8jivqN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_searchGR(model, word, input_char_index, output_char_index, reverse_input_char_index,\n",
        "                reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length,\n",
        "                num_encoder_tokens, num_decoder_tokens, beam_width, device, length_penalty=0.6):\n",
        "\n",
        "    # Encode the input word\n",
        "    data, word_t = np.zeros((max_encoder_seq_length, 1), dtype= d_type), ''\n",
        "    t_z, tab_pad, nl_pad,prob_dim, bw = 0, '\\t', '\\n', 1, beam_width\n",
        "    for t, char in enumerate(word):\n",
        "        data[t, 0] = input_char_index[char]\n",
        "    t_z = t+1\n",
        "    data[t_z :, 0] = input_char_index[c_end]\n",
        "\n",
        "    data = torch.tensor(data, dtype= td_type).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = model.encoder(data)\n",
        "\n",
        "    # Initialize beam\n",
        "    out_t = output_char_index[tab_pad]\n",
        "    out_reshape = np.array(out_t).reshape(1,)\n",
        "    hidden_par = hidden.unsqueeze(0)\n",
        "    initial_sequence = torch.tensor(out_reshape).to(device)\n",
        "    beam = [(0.0, initial_sequence, hidden_par)]  # [(score, sequence, hidden)]\n",
        "\n",
        "    for _ in range(max_decoder_seq_length):\n",
        "      score_cal , candidates = 0, []\n",
        "      for score, seq, hidden in beam:\n",
        "          score_cal = score_cal + score\n",
        "          last_token = seq[-1].item()\n",
        "          if last_token == output_char_index[nl_pad]:\n",
        "              # If the sequence ends with the end token, add it to the candidates\n",
        "              candidates.append((score, seq, hidden))\n",
        "              \n",
        "              continue\n",
        "          \n",
        "          last_t_reshape = np.array(last_token).reshape(1,)\n",
        "          hidden_par = hidden.squeeze(0)\n",
        "          x = torch.tensor(last_t_reshape).to(device)\n",
        "          output, hidden = model.decoder(x, hidden_par)\n",
        "          probabilities = F.softmax(output, dim = prob_dim)\n",
        "\n",
        "          # Get the top-k probabilities and tokens\n",
        "          topk_probs, topk_tokens = torch.topk(probabilities, k= bw)\n",
        "          cal_score = score\n",
        "          for prob, token in zip(topk_probs[0], topk_tokens[0]):\n",
        "              cal_score = cal_score + prob\n",
        "              new_seq = torch.cat((seq, token.unsqueeze(0)), dim=0)\n",
        "              n_hidden = hidden.clone()\n",
        "              new_seq_ln  = len(new_seq)\n",
        "              pen_fac = (new_seq_ln - 1) / 5\n",
        "              new_hidden = n_hidden.unsqueeze(0)\n",
        "              #length_penalty_factor = ((pen_fac) ** length_penalty)  # Adjust penalty factor as needed\n",
        "              \n",
        "              candidates.append((score + torch.log(prob).item() / ((pen_fac) ** length_penalty), new_seq, new_hidden))\n",
        "              cal_score = 0\n",
        "      # Select top-k candidates based on the accumulated scores\n",
        "      beam = heapq.nlargest(beam_width, candidates, key = lambda x: x[0])\n",
        "\n",
        "    \n",
        "    best_score, best_sequence, _ = max(beam, key=lambda x: x[0])# Select the best sequence from the beam as the output\n",
        "    \n",
        "    word_t = ''.join([reverse_target_char_index[token.item()] for token in best_sequence[1:-1]])\n",
        "\n",
        "    return word_t"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.349149Z",
          "iopub.execute_input": "2023-05-17T12:52:33.349864Z",
          "iopub.status.idle": "2023-05-17T12:52:33.370498Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.349831Z",
          "shell.execute_reply": "2023-05-17T12:52:33.369373Z"
        },
        "trusted": true,
        "id": "s_Bv6TU-kWDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, batch_size, learning_rate = 2, 32, 0.001\n",
        "load_model, training= False, False\n",
        "input_size_encoder, input_size_decoder = num_encoder_tokens, num_decoder_tokens\n",
        "hidden_size = 256  # Needs to be the same for both RNN's\n",
        "output_size = num_decoder_tokens\n",
        "dec_dropout, enc_dropout = 0.1, 0.1\n",
        "encoder_embedding_size, decoder_embedding_size = 256, 256\n",
        "num_dec_layers,num_enc_layers = 2, 2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.394192Z",
          "iopub.execute_input": "2023-05-17T12:52:33.394891Z",
          "iopub.status.idle": "2023-05-17T12:52:33.406200Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.394858Z",
          "shell.execute_reply": "2023-05-17T12:52:33.405145Z"
        },
        "trusted": true,
        "id": "f2iN4tbdkWDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(num_encoder_tokens,input_embedding_size, dp, cell_type, hidden_size, num_enc_layers, num_dec_layers,num_epochs,output_size,input_size_decoder,batch_size,beam_width):\n",
        "    #Decoder Selection\n",
        "    if(cell_type==\"LSTM\"):\n",
        "        decoder_net = Decoder(input_size_decoder,input_embedding_size,hidden_size,output_size,num_dec_layers,dp).to(device)\n",
        "    elif(cell_type==\"GRU\"):\n",
        "        decoder_net = DecoderGRU(input_size_decoder,decoder_embedding_size,hidden_size,output_size,num_dec_layers,dec_dropout).to(device)\n",
        "    else:\n",
        "        decoder_net = DecoderRNN(input_size_decoder,decoder_embedding_size,hidden_size,output_size,num_dec_layers,dec_dropout).to(device)\n",
        "    #Encoder Selection\n",
        "    if(cell_type==\"LSTM\"):\n",
        "        encoder_net = Encoder(input_size_encoder,input_embedding_size, hidden_size, num_enc_layers,dp).to(device)\n",
        "    elif(cell_type==\"GRU\"):\n",
        "        encoder_net = EncoderGRU(input_size_encoder, encoder_embedding_size, hidden_size, num_enc_layers, enc_dropout).to(device)\n",
        "    else:\n",
        "        encoder_net = EncoderRNN(input_size_encoder, encoder_embedding_size, hidden_size, num_enc_layers, enc_dropout).to(device)\n",
        "    \n",
        "    #Model Selection\n",
        "    if(cell_type==\"LSTM\"):    \n",
        "        model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
        "    else:\n",
        "        model = Seq2SeqGR(encoder_net, decoder_net).to(device)\n",
        "    \n",
        "    split_dim, bs,  m_norm = 1, batch_size, 1\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    train_ds_y, train_ds_x = torch.split(target_data, bs, dim = split_dim), torch.split(input_data, bs, dim = split_dim)\n",
        "    correct_prediction  = 0\n",
        "    #print(train_ds_x)\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "        model.eval()\n",
        "        total_count = len(val_X)\n",
        "        model.train()\n",
        "        for i, (x,y) in enumerate(zip(train_ds_x,train_ds_y)):\n",
        "            # Get input and targets and get to cuda\n",
        "            target, inp_data = y.to(device), x.to(device)\n",
        "            # Forward prop\n",
        "            output = model(inp_data, target)\n",
        "            target, output = target[1:].reshape(-1),  output[1:].reshape(-1, output.shape[2])\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward() # Back prop\n",
        "            # Clip to avoid exploding gradient issues, makes sure grads are\n",
        "            # within a healthy range\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = m_norm)\n",
        "            # Gradient descent step\n",
        "            optimizer.step()\n",
        "        correct_pred, total_words, lstm_bw = 0, total_count, 1\n",
        "        model.eval()\n",
        "        for i in range(total_count):\n",
        "            if(cell_type==\"LSTM\"):\n",
        "                decoded_sentence = beam_search(model,val_X[i], input_char_index, output_char_index, reverse_input_char_index, reverse_target_char_index, \n",
        "                                                     max_encoder_seq_length, max_decoder_seq_length,num_encoder_tokens, num_decoder_tokens,lstm_bw,device)\n",
        "            else:\n",
        "                 decoded_sentence = beam_searchGR(model,val_X[i], input_char_index, output_char_index, reverse_input_char_index, reverse_target_char_index, \n",
        "                                                        max_encoder_seq_length, max_decoder_seq_length,num_encoder_tokens, num_decoder_tokens,beam_width,device)\n",
        "                 \n",
        "            if decoded_sentence == val_Y[i][1:-1]:\n",
        "                correct_pred = correct_pred + 1\n",
        "        test_accuracy = correct_pred / total_words\n",
        "        print(correct_pred / total_words)  # test_accuracy = correct_pred / total_words\n",
        "        wandb.log({'Val_accuracy' : test_accuracy*100})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.407571Z",
          "iopub.execute_input": "2023-05-17T12:52:33.408200Z",
          "iopub.status.idle": "2023-05-17T12:52:33.429881Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.408167Z",
          "shell.execute_reply": "2023-05-17T12:52:33.428780Z"
        },
        "trusted": true,
        "id": "sWLWrPwukWDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'parameters': {\n",
        "                   'dec_num_layers':{\n",
        "                      'values': [1,2,3] \n",
        "                      },\n",
        "                   'embedding_size': {\n",
        "                      'values': [128, 256, 512] \n",
        "                      },\n",
        "                   'hidden_size': {\n",
        "                      'values': [128, 256, 512]\n",
        "                      },\n",
        "                   'dropout': {\n",
        "                      'values': [0.1, 0.2, 0.3, 0.4]\n",
        "                      },\n",
        "                   'cell_type': {\n",
        "                      'values': ['RNN','LSTM','GRU']\n",
        "                      },\n",
        "                   'beam_search':{\n",
        "                      'values':[1,2,3,4,5]\n",
        "                      },\n",
        "                   'epochs' :{\n",
        "                      'values':[10,20,30,40]\n",
        "                      },\n",
        "                   'enc_num_layers': {\n",
        "                      'values': [1,2,3]\n",
        "                      },\n",
        "                   'batch_size': {\n",
        "                      'values': [128,256,512]\n",
        "                      }\n",
        "                },\n",
        "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'}\n",
        "}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.431366Z",
          "iopub.execute_input": "2023-05-17T12:52:33.431981Z",
          "iopub.status.idle": "2023-05-17T12:52:33.443000Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.431950Z",
          "shell.execute_reply": "2023-05-17T12:52:33.441986Z"
        },
        "trusted": true,
        "id": "SD6WwN4akWDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    var1 = wandb.init(project=\"Assignment3_final\")\n",
        "    var2 = var1.config\n",
        "    # var2 is a variable that holds and saves hyperparameters and inputs\n",
        "    wandb.run.name = 'Cell_Type:-' + var2.cell_type + ', Batch_Size:-' + str(var2.batch_size) + ', Epochs:-' + str(var2.epochs) + ', Dropout:-' + str(var2.dropout) + ', Beam_Search:-' + str(var2.beam_search) +', Embedding_Size:-' + str(var2.embedding_size) + ', Hidden_Size:-' + str(var2.hidden_size) + ', Enc_Num_Layers:-' + str(var2.enc_num_layers) + ', Dec_Num_Layers:-' + str(var2.dec_num_layers)\n",
        "    if(var2.cell_type == 'RNN'):\n",
        "        epochs = 10\n",
        "        training(input_size_encoder ,var2.embedding_size, var2.dropout, var2.cell_type, var2.hidden_size, var2.enc_num_layers,  var2.enc_num_layers,epochs,num_decoder_tokens,num_decoder_tokens,var2.batch_size,var2.beam_search)\n",
        "    else:\n",
        "        training(input_size_encoder ,var2.embedding_size, var2.dropout, var2.cell_type, var2.hidden_size, var2.enc_num_layers,  var2.enc_num_layers,var2.epochs,num_decoder_tokens,num_decoder_tokens,var2.batch_size,var2.beam_search)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:52:33.444385Z",
          "iopub.execute_input": "2023-05-17T12:52:33.444918Z",
          "iopub.status.idle": "2023-05-17T12:52:33.457678Z",
          "shell.execute_reply.started": "2023-05-17T12:52:33.444887Z",
          "shell.execute_reply": "2023-05-17T12:52:33.456637Z"
        },
        "trusted": true,
        "id": "GLFi6HUmkWDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"Assignment3_final\")\n",
        "wandb.agent(sweep_id, train, count=40)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T12:53:07.727651Z",
          "iopub.execute_input": "2023-05-17T12:53:07.729967Z",
          "iopub.status.idle": "2023-05-17T13:22:33.534849Z",
          "shell.execute_reply.started": "2023-05-17T12:53:07.729930Z",
          "shell.execute_reply": "2023-05-17T13:22:33.533777Z"
        },
        "trusted": true,
        "id": "MVw2KTdOkWDk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
