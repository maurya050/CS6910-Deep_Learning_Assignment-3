{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "!pip install wandb\n",
        "import wandb\n",
        "import heapq\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.ticker as ticker\n",
        "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-05-17T13:10:40.741467Z",
          "iopub.execute_input": "2023-05-17T13:10:40.742394Z",
          "iopub.status.idle": "2023-05-17T13:10:52.578098Z",
          "shell.execute_reply.started": "2023-05-17T13:10:40.742316Z",
          "shell.execute_reply": "2023-05-17T13:10:52.576879Z"
        },
        "trusted": true,
        "id": "2LEl6PNXZ0qe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "032a8b9a-7a70-4efc-80a1-83259af2b0e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.23.1-py2.py3-none-any.whl (205 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=451b2014d9fe26d2c2721e053166cb1f2b32bc4304fab6829a5472827fed2c91\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.23.1 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOJqiGKBaGbT",
        "outputId": "fa7341d8-6020-46d5-cd72-7d9e53865453"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/aksharantar_dataset/tel/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbhfRVCZyi2E",
        "outputId": "f3996db9-4eb4-4848-802f-1e069539281c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/aksharantar_dataset/tel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tsv_file = open(\"/content/drive/MyDrive/aksharantar_dataset/tel/tel_train.csv\")\n",
        "test_tsv_file = open(\"/content/drive/MyDrive/aksharantar_dataset/tel/tel_test.csv\")\n",
        "val_tsv_file = open(\"/content/drive/MyDrive/aksharantar_dataset/tel/tel_valid.csv\")\n",
        "read_tsv = csv.reader(tsv_file)\n",
        "test_read_tsv = csv.reader(test_tsv_file)\n",
        "val_read_tsv = csv.reader(val_tsv_file)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T13:10:52.580212Z",
          "iopub.execute_input": "2023-05-17T13:10:52.580939Z",
          "iopub.status.idle": "2023-05-17T13:10:52.586449Z",
          "shell.execute_reply.started": "2023-05-17T13:10:52.580900Z",
          "shell.execute_reply": "2023-05-17T13:10:52.585206Z"
        },
        "trusted": true,
        "id": "rBanAoluZ0qi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, train_Y = [], []\n",
        "for i in read_tsv:\n",
        "    train_X.append(i[0])\n",
        "    train_Y.append(i[1])\n",
        "    \n",
        "test_X,test_Y = [], []\n",
        "for i in test_read_tsv:\n",
        "    test_X.append(i[0])\n",
        "    test_Y.append(i[1])\n",
        "    \n",
        "val_X, val_Y = [], []\n",
        "for i in val_read_tsv:\n",
        "    val_X.append(i[0])\n",
        "    val_Y.append(i[1])\n",
        "    \n",
        "\n",
        "\n",
        "test_X, test_Y = np.array(test_X), np.array(test_Y)\n",
        "t_range = test_Y.shape[0]\n",
        "val_X, val_Y = np.array(val_X),  np.array(val_Y)\n",
        "v_range = val_Y.shape[0]\n",
        "train_X, train_Y = np.array(train_X), np.array(train_Y)\n",
        "tr_range = train_Y.shape[0]\n",
        "for i in range(t_range):\n",
        "    test_Y[i] = \"\\t\" + test_Y[i] + \"\\n\"\n",
        "    \n",
        "for i in range(v_range):\n",
        "    val_Y[i] = \"\\t\" + val_Y[i] + \"\\n\"\n",
        "\n",
        "for i in range(tr_range):\n",
        "    train_Y[i] = \"\\t\" + train_Y[i] + \"\\n\"\n",
        "ch_end = \" \""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T13:10:52.588557Z",
          "iopub.execute_input": "2023-05-17T13:10:52.589072Z",
          "iopub.status.idle": "2023-05-17T13:10:52.870757Z",
          "shell.execute_reply.started": "2023-05-17T13:10:52.589006Z",
          "shell.execute_reply": "2023-05-17T13:10:52.869778Z"
        },
        "trusted": true,
        "id": "t4XI1NCaZ0qi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_input_corpus, output_corpus, input_corpus,  val_output_corpus = set(), set(), set(), set()\n",
        "\n",
        "for word in train_Y:\n",
        "    for char in word:\n",
        "        if char not in output_corpus:\n",
        "            output_corpus.add(char)\n",
        "\n",
        "for word in train_X:\n",
        "    for char in word:\n",
        "        if char not in input_corpus:\n",
        "            input_corpus.add(char)\n",
        "\n",
        "output_corpus.add(ch_end)\n",
        "input_corpus.add(ch_end)\n",
        "\n",
        "\n",
        "for word in val_Y:\n",
        "    for char in word:\n",
        "        if char not in val_output_corpus:\n",
        "            val_output_corpus.add(char)\n",
        "\n",
        "output_corpus,  input_corpus = sorted(list(output_corpus)), sorted(list(input_corpus))\n",
        "num_encoder_tokens =  len(input_corpus)  \n",
        "\n",
        "for word in val_X:\n",
        "    for char in word:\n",
        "        if char not in val_input_corpus:\n",
        "            val_input_corpus.add(char)\n",
        "\n",
        "num_decoder_tokens = len(output_corpus)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T13:10:52.873994Z",
          "iopub.execute_input": "2023-05-17T13:10:52.874337Z",
          "iopub.status.idle": "2023-05-17T13:10:53.233021Z",
          "shell.execute_reply.started": "2023-05-17T13:10:52.874305Z",
          "shell.execute_reply": "2023-05-17T13:10:53.231840Z"
        },
        "trusted": true,
        "id": "MqHk0njfZ0qk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_decoder_seq_length,max_encoder_seq_length = max([len(txt) for txt in train_Y]), max([len(txt) for txt in train_X]) + 2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T13:10:53.245617Z",
          "iopub.execute_input": "2023-05-17T13:10:53.246376Z",
          "iopub.status.idle": "2023-05-17T13:10:53.256591Z",
          "shell.execute_reply.started": "2023-05-17T13:10:53.246344Z",
          "shell.execute_reply": "2023-05-17T13:10:53.255162Z"
        },
        "trusted": true,
        "id": "iVEOiAJ1Z0ql"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "print(\"Number of samples:\", len(train_X))\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T13:10:53.258303Z",
          "iopub.execute_input": "2023-05-17T13:10:53.258778Z",
          "iopub.status.idle": "2023-05-17T13:10:53.270246Z",
          "shell.execute_reply.started": "2023-05-17T13:10:53.258709Z",
          "shell.execute_reply": "2023-05-17T13:10:53.268963Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IQYyS8sZ0qm",
        "outputId": "b598bb11-0a5a-4704-b458-6833ecc3f985"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique input tokens: 27\n",
            "Max sequence length for outputs: 21\n",
            "Number of samples: 51200\n",
            "Max sequence length for inputs: 30\n",
            "Number of unique output tokens: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, num_epochs, learning_rate = 32, 10, 0.001\n",
        "load_model, training = False, False\n",
        "input_size_encoder, input_size_decoder = num_encoder_tokens, num_decoder_tokens\n",
        "output_size = num_decoder_tokens\n",
        "encoder_embedding_size, decoder_embedding_size = 256, 256\n",
        "hidden_size = 256  # Needs to be the same for both RNN's\n",
        "num_enc_layers, num_dec_layers = 1, 1\n",
        "enc_dropout, dec_dropout = 0.1, 0.1\n",
        "d_type, td_type = \"int64\", torch.int64"
      ],
      "metadata": {
        "id": "MXyIQzVwzvju"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ln = len(train_X)\n",
        "val_ln = len(val_X)\n",
        "input_char_index, output_char_index = dict([(char, i) for i, char in enumerate(input_corpus)]), dict([(char, i) for i, char in enumerate(output_corpus)])\n",
        "input_data, target_data = np.zeros((max_encoder_seq_length,train_ln), dtype= d_type), np.zeros((max_decoder_seq_length,train_ln), dtype=d_type)\n",
        "c_end = \" \"\n",
        "input_data_val, target_data_val = np.zeros((max_encoder_seq_length,val_ln), dtype=d_type), np.zeros((max_decoder_seq_length,val_ln), dtype=d_type)\n",
        "for i, (x, y) in enumerate(zip(val_X, val_Y)):\n",
        "    t_n = i+ val_ln\n",
        "    for t, char in enumerate(y):\n",
        "        target_data_val[t, i] = output_char_index[char]\n",
        "    t_n = t+1       \n",
        "    target_data_val[t_n :,i] = output_char_index[c_end]\n",
        "    t_n = 0\n",
        "    for t, char in enumerate(x):\n",
        "        input_data_val[t, i] = input_char_index[char]\n",
        "    t_n = t+1   \n",
        "    input_data_val[t_n :,i] = input_char_index[c_end] \n",
        "t_n =0    \n",
        "for i, (x, y) in enumerate(zip(train_X, train_Y)):\n",
        "    t_n = i+ val_ln\n",
        "    for t, char in enumerate(y):\n",
        "        target_data[t, i] = output_char_index[char]\n",
        "    t_n  = t+1        \n",
        "    target_data[t_n :,i] = output_char_index[c_end]\n",
        "    t_n =0\n",
        "    for t, char in enumerate(x):\n",
        "        input_data[t, i] = input_char_index[char]\n",
        "    t_n = t+1   \n",
        "    input_data[t_n :,i] = input_char_index[c_end]"
      ],
      "metadata": {
        "id": "TojY0s7Jzya1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convertin numpy arrays to tensors\n",
        "input_data_val, target_data_val = torch.tensor(input_data_val,dtype = td_type), torch.tensor(target_data_val,dtype = td_type)\n",
        "input_data, target_data = torch.tensor(input_data,dtype = td_type), torch.tensor(target_data,dtype = td_type)"
      ],
      "metadata": {
        "id": "YwONTFxlz1v2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_target_char_index,reverse_input_char_index, t_z = dict((i, char) for char, i in output_char_index.items()), dict((i, char) for char, i in input_char_index.items()), 0"
      ],
      "metadata": {
        "id": "dnD2THejz4l1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module): \n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        drop_par = dropout\n",
        "        self.dropout = nn.Dropout(drop_par)\n",
        "        self.num_layers, self.hidden_size, = num_layers, hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n",
        "\n",
        "    def forward(self, x): # x shape: (seq_length, N) where N is batch size\n",
        "        drop_par = self.embedding(x)\n",
        "        embedding = self.dropout(drop_par) # embedding shape: (seq_length, N, embedding_size)\n",
        "        outputs, (hidden, cell) = self.rnn(self.dropout(drop_par)) # outputs shape: (seq_length, N, hidden_size)\n",
        "        return hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder):\n",
        "\n",
        "        super(Seq2Seq, self).__init__()  \n",
        "        self.decoder, self.encoder = decoder, encoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.2):\n",
        "        batch_size, target_len, target_vocab_size = source.shape[1], target.shape[0], num_decoder_tokens\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "        # Grab the first input to the Decoder which will be <SOS> token\n",
        "        x = target[0]\n",
        "        hidden, cell = self.encoder(source)\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            # Use previous hidden, cell as context from encoder at start\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "\n",
        "            # Store next output prediction\n",
        "            outputs[t], best_guess = output, output.argmax(1)\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            x = best_guess if random.random() >= teacher_force_ratio else target[t]\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        drop_par, hidden_layer_size = dropout, hidden_size\n",
        "        self.dropout,self.num_layers, self.hidden_size = nn.Dropout(drop_par),  num_layers, hidden_layer_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout= drop_par)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell): # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
        "        # is 1 here because we are sending in a single word and not a sentence\n",
        "        x  = x.unsqueeze(0) \n",
        "        drop_par = self.embedding(x)\n",
        "        embedding = self.dropout(drop_par) # embedding shape: (1, N, embedding_size)\n",
        "        outputs, (hidden, cell) = self.rnn(self.dropout(drop_par), (hidden, cell)) # outputs shape: (1, N, hidden_size)\n",
        "        #predictions = self.fc(outputs)\n",
        "        \n",
        "        \n",
        "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
        "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
        "        # just gonna remove the first dim\n",
        "        predictions = self.fc(outputs).squeeze(0)\n",
        "\n",
        "        return predictions, hidden, cell"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T14:18:02.578338Z",
          "iopub.execute_input": "2023-05-17T14:18:02.578742Z",
          "iopub.status.idle": "2023-05-17T14:18:02.600690Z",
          "shell.execute_reply.started": "2023-05-17T14:18:02.578712Z",
          "shell.execute_reply": "2023-05-17T14:18:02.599362Z"
        },
        "trusted": true,
        "id": "sbZHV9wLZ0qp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(model, word, input_char_index, output_char_index, reverse_input_char_index,reverse_target_char_index, max_encoder_seq_length, \n",
        "                max_decoder_seq_length,num_encoder_tokens, num_decoder_tokens, beam_width, device, length_penalty=0.6):\n",
        "\n",
        "\n",
        "    # Encode the input word\n",
        "    data, word_t = np.zeros((max_encoder_seq_length, 1), dtype= d_type), ''\n",
        "    t_z , bw = 0, beam_width\n",
        "    for t, char in enumerate(word):\n",
        "        data[t, 0] = input_char_index[char]\n",
        "    t_z = t+1\n",
        "    data[t_z:, 0] = input_char_index[c_end]\n",
        "\n",
        "    data = torch.tensor(data, dtype= td_type ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden,cell = model.encoder(data)\n",
        "\n",
        "    # Initialize beam\n",
        "    out_t = output_char_index['\\t']\n",
        "    out_reshape = np.array(out_t).reshape(1,)\n",
        "    hidden_par = hidden.unsqueeze(0)\n",
        "    initial_sequence = torch.tensor(out_reshape).to(device)\n",
        "    beam = [(0.0, initial_sequence, hidden_par)]  # [(score, sequence, hidden)]\n",
        "\n",
        "    for _ in range(max_decoder_seq_length):\n",
        "      candidates = []\n",
        "      for score, seq, hidden in beam:\n",
        "          # last_token = seq[-1].item()\n",
        "          if seq[-1].item() == output_char_index['\\n']:\n",
        "              # If the sequence ends with the end token, add it to the candidates\n",
        "              candidates.append((score, seq, hidden))\n",
        "              continue\n",
        "          lt_reshape = np.array(seq[-1].item()).reshape(1,) # last_token = seq[-1].item()\n",
        "          hidden_upar = hidden.squeeze(0)\n",
        "          x = torch.tensor(lt_reshape).to(device)\n",
        "          output, hidden,cell = model.decoder(x, hidden_upar,cell)\n",
        "          probabilities = F.softmax(output, dim=1)\n",
        "          cal_cand = 0\n",
        "          # Get the top-k probabilities and tokens\n",
        "          topk_probs, topk_tokens = torch.topk(probabilities, k=bw)\n",
        "\n",
        "          for prob, token in zip(topk_probs[0], topk_tokens[0]):\n",
        "              cal_cand = prob\n",
        "              n_hidden = hidden.clone()\n",
        "              new_seq = torch.cat((seq, token.unsqueeze(0)), dim=0)\n",
        "              ln_ns = len(new_seq)\n",
        "              ln_pf = ((ln_ns - 1) / 5)\n",
        "              new_hidden = n_hidden.unsqueeze(0)\n",
        "              #length_penalty_factor = ln_pf ** length_penalty  # Adjust penalty factor as needed\n",
        "              candidate_cal = score + torch.log(prob).item() / (ln_pf ** length_penalty)\n",
        "              candidates.append((candidate_cal, new_seq, new_hidden))\n",
        "\n",
        "      beam = heapq.nlargest(bw, candidates, key=lambda x: x[0])# Select top-k candidates based on the accumulated scores\n",
        "\n",
        "      \n",
        "    best_score, best_sequence, _ = max(beam, key=lambda x: x[0]) # Select the best sequence from the beam as the output\n",
        "    \n",
        "    cal_score = best_score\n",
        "    word_t = ''.join([reverse_target_char_index[token.item()] for token in best_sequence[1:-1]])\n",
        "    cal_score = 0\n",
        "    return word_t"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T14:18:02.774587Z",
          "iopub.execute_input": "2023-05-17T14:18:02.774970Z",
          "iopub.status.idle": "2023-05-17T14:18:02.791182Z",
          "shell.execute_reply.started": "2023-05-17T14:18:02.774938Z",
          "shell.execute_reply": "2023-05-17T14:18:02.789993Z"
        },
        "trusted": true,
        "id": "PudHrXV1Z0qq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, batch_size, learning_rate = 2, 32, 0.001\n",
        "load_model, training= False, False\n",
        "input_size_encoder, input_size_decoder = num_encoder_tokens, num_decoder_tokens\n",
        "hidden_size = 256  # Needs to be the same for both RNN's\n",
        "output_size = num_decoder_tokens\n",
        "dec_dropout, enc_dropout = 0.1, 0.1\n",
        "num_dec_layers,num_enc_layers = 2, 2\n",
        "encoder_embedding_size, decoder_embedding_size = 256, 256"
      ],
      "metadata": {
        "id": "Ph8zJAFTzTMP"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_encoder_tokens,input_embedding_size, dp, cell_, hidden_size, num_enc_layers, num_dec_layers,num_epochs, output_size, input_size_decoder,batch_size, beam_width):\n",
        "    \n",
        "      #Decoder Selection\n",
        "      if(cell_==\"LSTM\"):\n",
        "          decoder_net = Decoder(input_size_decoder,input_embedding_size,hidden_size,output_size,num_dec_layers,dp).to(device)\n",
        "            #Encoder Selection\n",
        "      if(cell_==\"LSTM\"):\n",
        "          encoder_net = Encoder(input_size_encoder,input_embedding_size, hidden_size, num_enc_layers,dp).to(device)\n",
        "     \n",
        "      #Model Selection\n",
        "      if(cell_==\"LSTM\"):    \n",
        "          model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
        "\n",
        "      split_dim, bs,  m_norm = 1, batch_size, 1\n",
        "      optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      train_ds_y, train_ds_x = torch.split(target_data, bs, dim = split_dim), torch.split(input_data, bs, dim = split_dim)\n",
        "      correct_prediction  = 0\n",
        "      #print(train_ds_x)\n",
        "      for epoch in range(num_epochs):\n",
        "          print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "          model.eval()\n",
        "          total_count = len(val_X)\n",
        "          model.train()\n",
        "          for i, (x,y) in enumerate(zip(train_ds_x,train_ds_y)):\n",
        "              # Get input and targets and get to cuda\n",
        "              target, inp_data = y.to(device), x.to(device)\n",
        "              # Forward prop\n",
        "              output = model(inp_data, target)\n",
        "              target, output = target[1:].reshape(-1),  output[1:].reshape(-1, output.shape[2])\n",
        "              optimizer.zero_grad()\n",
        "              loss = criterion(output, target)\n",
        "              loss.backward() # Back prop\n",
        "              # Clip to avoid exploding gradient issues, makes sure grads are\n",
        "              # within a healthy range\n",
        "              torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = m_norm)\n",
        "              # Gradient descent step\n",
        "              optimizer.step()\n",
        "          correct_pred, total_, lstm_bw = 0, total_count, 1\n",
        "          model.eval()\n",
        "          for i in range(total_):\n",
        "              if(cell_==\"LSTM\"):\n",
        "                  decoded_sentence = beam_search(model,val_X[i], input_char_index, output_char_index, reverse_input_char_index, reverse_target_char_index, \n",
        "                                                 max_encoder_seq_length, max_decoder_seq_length, num_encoder_tokens, num_decoder_tokens,1,device)\n",
        "              if decoded_sentence == val_Y[i][1:-1]:\n",
        "                  correct_pred = correct_pred + 1\n",
        "          test_accuracy = correct_pred / total_\n",
        "          print(correct_pred / total_)  # test_accuracy = correct_pred / total_w\n",
        "      \n",
        "      correct_pred, total_words = 0,  len(test_X)\n",
        "      dec, cor = [], []\n",
        "      for i in range(total_words):\n",
        "          decoded_sentence = beam_search(model,test_X[i], input_char_index, output_char_index, reverse_input_char_index, reverse_target_char_index,\n",
        "                              max_encoder_seq_length, max_decoder_seq_length, num_encoder_tokens, num_decoder_tokens,1,device)\n",
        "          if decoded_sentence == test_Y[i][1:-1]:\n",
        "                correct_pred = correct_pred + 1\n",
        "      test_accuracy = correct_pred / total_words\n",
        "      ex, inp =  [], []\n",
        "      np.random.seed(10)\n",
        "      print(\"Test_Accuracy is : \")\n",
        "      print(correct_pred / total_words)\n",
        "      for index in range(total_words):\n",
        "          decoded_sentence = beam_search(model,test_X[index], input_char_index, output_char_index, reverse_input_char_index, reverse_target_char_index,\n",
        "                               max_encoder_seq_length, max_decoder_seq_length, num_encoder_tokens, num_decoder_tokens,1,device)\n",
        "\n",
        "          if test_Y[index][1:-1] != decoded_sentence:\n",
        "              ex.append(\"NO\")\n",
        "          else:\n",
        "              ex.append(\"YES\")        \n",
        "          \n",
        "          dec_out, inp_word = decoded_sentence, test_X[index]\n",
        "          inp.append(inp_word)\n",
        "          true_out = test_Y[index][1:-1]\n",
        "          dec.append(dec_out)\n",
        "          cor.append(true_out)\n",
        "\n",
        "      grid = {\n",
        "              'Input_Word': inp, \n",
        "              'Decoded_Output' : dec, \n",
        "              'True_Output' : cor,\n",
        "              \"Match Result\" : ex\n",
        "             }\n",
        "      csv_path = '/content/drive/MyDrive/aksharantar_dataset/tel/Van_Pred.csv'\n",
        "      df = pd.DataFrame(grid)\n",
        "      df.to_csv(csv_path, index=False,header=True)\n",
        "      pd.DataFrame(grid)\n",
        "      print(df.head(20))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T14:18:03.161096Z",
          "iopub.execute_input": "2023-05-17T14:18:03.161726Z",
          "iopub.status.idle": "2023-05-17T14:18:03.169849Z",
          "shell.execute_reply.started": "2023-05-17T14:18:03.161692Z",
          "shell.execute_reply": "2023-05-17T14:18:03.168073Z"
        },
        "trusted": true,
        "id": "64lQkl50Z0qs"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(input_size_encoder ,256, 0.3, \"LSTM\",512, 3, 3, 30, num_decoder_tokens, num_decoder_tokens, 512, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYns5AO3Z0qy",
        "outputId": "b441b547-e02f-4456-d9a2-2cf2e77a8323"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0 / 30]\n",
            "0.0\n",
            "[Epoch 1 / 30]\n",
            "0.0\n",
            "[Epoch 2 / 30]\n",
            "0.0\n",
            "[Epoch 3 / 30]\n",
            "0.010986328125\n",
            "[Epoch 4 / 30]\n",
            "0.103271484375\n",
            "[Epoch 5 / 30]\n",
            "0.24951171875\n",
            "[Epoch 6 / 30]\n",
            "0.351806640625\n",
            "[Epoch 7 / 30]\n",
            "0.39697265625\n",
            "[Epoch 8 / 30]\n",
            "0.421630859375\n",
            "[Epoch 9 / 30]\n",
            "0.46044921875\n",
            "[Epoch 10 / 30]\n",
            "0.487060546875\n",
            "[Epoch 11 / 30]\n",
            "0.49853515625\n",
            "[Epoch 12 / 30]\n",
            "0.4931640625\n",
            "[Epoch 13 / 30]\n",
            "0.5009765625\n",
            "[Epoch 14 / 30]\n",
            "0.514892578125\n",
            "[Epoch 15 / 30]\n",
            "0.499755859375\n",
            "[Epoch 16 / 30]\n",
            "0.507080078125\n",
            "[Epoch 17 / 30]\n",
            "0.5419921875\n",
            "[Epoch 18 / 30]\n",
            "0.52001953125\n",
            "[Epoch 19 / 30]\n",
            "0.521728515625\n",
            "[Epoch 20 / 30]\n",
            "0.52685546875\n",
            "[Epoch 21 / 30]\n",
            "0.556640625\n",
            "[Epoch 22 / 30]\n",
            "0.560302734375\n",
            "[Epoch 23 / 30]\n",
            "0.555419921875\n",
            "[Epoch 24 / 30]\n",
            "0.5625\n",
            "[Epoch 25 / 30]\n",
            "0.561767578125\n",
            "[Epoch 26 / 30]\n",
            "0.55419921875\n",
            "[Epoch 27 / 30]\n",
            "0.5439453125\n",
            "[Epoch 28 / 30]\n",
            "0.546630859375\n",
            "[Epoch 29 / 30]\n",
            "0.56396484375\n",
            "Test_Accuracy is : \n",
            "0.508544921875\n",
            "           Input_Word Decoded_Output   True_Output Match Result\n",
            "0          vithananni       వితనన్ని   విత్తనాన్ని           NO\n",
            "1        prayaanikulu    ప్రయాణికులు   ప్రయాణికులు          YES\n",
            "2              hassan         హసససన్          హసన్           NO\n",
            "3            pakshala          పక్షల        పక్షాల           NO\n",
            "4             goutham           గౌతం         గౌతమ్           NO\n",
            "5       puraanamulanu     పురాణములను    పురాణములను          YES\n",
            "6               union        యునియన్       యూనియన్           NO\n",
            "7             medassu        మెదస్సు       మేధస్సు           NO\n",
            "8             kuantum         కుంంటు      క్వాంటమ్           NO\n",
            "9   aakaashaannayinaa   ఆకాశాన్నిియా  ఆకాశాన్నయినా           NO\n",
            "10              naats         నాట్స్        నాట్స్          YES\n",
            "11            jaavaed         జావెడ్        జావేద్           NO\n",
            "12             powell         పావ్ల్        పావెల్           NO\n",
            "13        deshamunaku       దేశమునకు      దేశమునకు          YES\n",
            "14             dublin        డబ్లిన్      డుబ్లిన్           NO\n",
            "15            cholera         చోలెరా          కలరా           NO\n",
            "16           vatakara          వాటకర          వడగర           NO\n",
            "17              silva         సిల్వా        సిల్వా          YES\n",
            "18             patter          పటట్ర         పాటర్           NO\n",
            "19    praatipadikapai   ప్రాతిపడికపై  ప్రాతిపదికపై           NO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FQ48j2yZqT5E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}